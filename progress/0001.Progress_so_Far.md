# Project Status: AWS Capacity Forecaster
**Date:** 2026-01-31
**Status:** Active Development

## 1. Core Architecture Implemented
The modular pipeline architecture is established in `src/modules/`, breaking down the forecasting process into distinct stages:

*   **Module 00: Pipeline Runner**
    *   Orchestration logic to run the full pipeline or individual steps.
*   **Module 01: Data Generation**
    *   Synthetic data generation logic (`module_01_data_generation.py`) to simulate server metrics when real data is unavailable.
*   **Module 02: Data Loading**
    *   Mechanisms to load data from local sources or S3 (`module_02_data_load.py`).
*   **Module 03: ETL & Feature Engineering**
    *   Data cleaning and feature preparation (`module_03_etl_feature_eng.py`) to get data ready for modeling.
*   **Module 04: Model Training (Current Focus)**
    *   **Wrapper (`module_04_model_training.py`)**: A robust wrapper that handles:
        *   **Dependency Management**: Auto-installs a complex stack of dependencies (AutoGluon, Prophet, NumPy < 2.0, etc.) at runtime if needed.
        *   **Environment Detection**: Detects if running Locally vs. SageMaker (`/opt/ml` detection) and adjusts paths accordingly.
    *   **Inner Logic (`module_04_inner.py`)**: The core training script (encapsulated) where the actual model training occurs.
*   **Module 05: Risk & Capacity Analysis**
    *   Logic to interpret model outputs and assess capacity risks (`module_05_risk_capacity_analysis.py`).

## 2. Infrastructure & Environment
*   **VS Code Configuration**:
    *   Configured `.vscode/settings.json` to automatically activate the correct Python environment using relative paths or explicit venv paths, ensuring consistency across machines.
    *   Verified git tracking policies for `.vscode` settings.
*   **PowerShell Integration**:
    *   Investigated and confirmed auto-activation behavior in PowerShell terminals.
*   **Dependency Management**:
    *   Resolved complex version conflicts between AutoGluon, Pandas 2.x, and NumPy < 2.0.
    *   Created a `requirements.txt` snapshot.

## 3. SageMaker & Cloud Integration
*   **Deployment Scripts**:
    *   `scripts/setup_sagemaker_role.py`: Configures IAM roles.
    *   `scripts/push_to_s3.py`: Uploads code/data artifacts to S3.
    *   `scripts/deploy_sagemaker_job.py`: Submits processing jobs to AWS SageMaker.
    *   `scripts/get_sagemaker_logs.py`: Fetches CloudWatch logs for debugging.

## 4. Notebooks & Exploration
*   **EDA**: `notebooks/02b_eda_validation.ipynb` exists for validating input data distributions.
*   **Pipeline Prototypes**: Jupyter notebooks (`01` through `05`) mirror the module structure for interactive debugging and testing.