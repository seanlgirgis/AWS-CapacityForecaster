## Project Documentation: Scripts & Jobs Overview

This document outlines the end-to-end scripts/jobs for **AWS-CapacityForecaster**, structured as modular Python components to simulate Citi-style enterprise capacity planning pipelines. Each script/job focuses on a phase of the workflow: data generation, ETL, ML forecasting, risk analysis/optimization, visualization/reporting, and optional automation/deployment.

The design ensures **complete coverage** of technologies from **citi.md** (e.g., pandas for ETL/data manipulation, numpy for numerical ops, scikit-learn for regression/clustering/imputation, Prophet for time-series, matplotlib/seaborn/plotly for viz, sqlalchemy for SQL queries, scipy for stats/percentiles, statsmodels for ADF tests, joblib for parallel processing, dash for optional dashboards, BeautifulSoup for any scraping if needed, multiprocessing for speed, requests/logging for utils) and **00002.aws_added_To_Project1.md** (e.g., boto3 for S3 uploads, SageMaker for ML training/notebooks/processing jobs, Athena for SQL querying, Amazon Forecast optional integration, QuickSight for dashboards, Lambda/EventBridge for automation, with cost controls like small instances/spot).

Scripts are designed as .py files in `src/` for reusability/production, with Jupyter notebooks in `notebooks/` for exploration. All integrate AWS via boto3/SageMaker SDK.

### Suggested Directory Structure
This structure follows standard Python/ML best practices: modular src, exploratory notebooks, tests, configs, and deployment scripts. It supports CI/CD (e.g., GitHub Actions) and AWS deployment.

```
C:\pyproj\AWS-CapacityForecaster\
├── src\                        # Main Python code modules
│   ├── __init__.py             # Makes src a package
│   ├── data_generation.py      # Synthetic data gen
│   ├── etl_pipeline.py         # Data cleansing/feature eng
│   ├── ml_forecasting.py       # Model training/comparison
│   ├── risk_analysis.py        # Risk flagging/optimization
│   ├── visualization.py        # Plots/reports/dashboards
│   └── utils\                  # Helper modules
│       ├── __init__.py
│       ├── aws_utils.py        # boto3 wrappers (S3, SageMaker, Athena)
│       ├── data_utils.py       # Common pandas/numpy/sqlalchemy funcs
│       ├── ml_utils.py         # Model eval, joblib parallel
│       └── config.py           # YAML/config loading
├── notebooks\                  # Exploratory Jupyter notebooks
│   ├── 01_data_generation.ipynb
│   ├── 02_etl_pipeline.ipynb
│   ├── 03_ml_forecasting.ipynb
│   ├── 04_risk_analysis.ipynb
│   └── 05_visualization.ipynb
├── tests\                      # Unit/integration tests
│   ├── test_data_generation.py
│   ├── test_etl_pipeline.py
│   └── ... (one per src script)
├── config\                     # Config files
│   └── config.yaml             # AWS creds, params (e.g., bucket name, instance types)
├── scripts\                    # Deployment/automation scripts
│   ├── deploy_sagemaker_job.py # Trigger SageMaker jobs
│   ├── lambda_handler.py       # Optional Lambda for scheduling
│   └── setup_environment.ps1   # PS1 to create structure (below)
├── docs\                       # Existing docs (SRS, Architecture, DataSchema, etc.)
│   └── images\                 # Diagrams/PNGs
├── requirements.txt            # Pip dependencies
├── README.md                   # Project overview, setup instructions
├── .gitignore                  # Ignore __pycache__, .env, etc.
└── .env                        # Local env vars (AWS keys - not committed)
```

### Suggested Helper Libraries (for Production Support)
These are utility libraries to make the project robust, maintainable, and production-ready (beyond core ML/data libs from citi.md/00002):
- **logging**: For structured logs (e.g., INFO/ERROR levels, file rotation) — used across all scripts for traceability.
- **yaml/pyyaml**: For config loading (e.g., AWS params, model hyperparameters) — flexible, human-readable.
- **argparse**: CLI args for scripts (e.g., run etl_pipeline.py --bucket my-bucket).
- **pytest**: For tests/ folder — unit test coverage, ensures success criteria.
- **boto3**: Core AWS integration (already in 00002) — wrapped in utils/aws_utils.py for reusability.
- **pyarrow**: For Parquet read/write (efficient with pandas, Athena-optimized).
- **dotenv**: Load .env vars (e.g., AWS_ACCESS_KEY) — secure local dev.
- **requests**: For any API calls (e.g., if extending to external data).
- **multiprocessing**: Parallel tasks (from citi.md, for large sims).

Install via `requirements.txt`:
```
pandas
numpy
scikit-learn
prophet
matplotlib
seaborn
plotly
sqlalchemy
scipy
statsmodels
joblib
dash
beautifulsoup4
multiprocessing
boto3
pyarrow
pyyaml
argparse
pytest
python-dotenv
openpyxl  # For Excel reports
reportlab  # For PDF reports
sagemaker  # AWS SDK for SageMaker
```

### Scripts/Jobs Overview (Begin to End)
Workflow: 1. Data Gen → 2. ETL → 3. ML Forecasting → 4. Risk/Optimization → 5. Viz/Reporting → (Optional) Automation.

Each maps citi.md skills (e.g., ETL with pandas/numpy/sqlalchemy, ML with scikit-learn/Prophet/statsmodels/joblib, viz with matplotlib/seaborn/plotly/dash, cleansing with scipy/imputation, parallel with joblib/multiprocessing) and 00002 stack (boto3/S3 uploads, SageMaker jobs, Athena queries, Amazon Forecast optional, QuickSight export, Lambda triggers).

1. **data_generation.py**  
   **Function**: Generate synthetic server metrics (100+ servers, 3+ years daily P95 data with seasonality/peaks) to simulate Citi monitoring feeds (e.g., BMC TrueSight). Covers citi.md's data integration/pipelines.  
   **Inputs**: Config params (num_servers, years, seasonality flags) from config.yaml; optional external regressors (e.g., banking holidays).  
   **Outputs**: CSVs uploaded to S3 raw layer (partitioned by year/month/day); local CSVs for testing.  
   **Success Criteria**: 100k+ rows generated <1 min; validates P95 ranges (0-100%); Athena queryable post-upload; no missing values in core columns.  
   **Mapped Libraries/Skills**: pandas (DataFrames, groupby, CSV write), numpy (vectorized ops, random noise for realism), scipy (percentile sims), joblib/multiprocessing (parallel gen for speed), boto3 (S3 upload from 00002).

2. **etl_pipeline.py**  
   **Function**: Ingest raw S3 data, cleanse (missing/outliers), feature eng (lags/rolling), enrich (anomaly detection), output Parquet to processed S3. Simulates Citi's data pipelines/Oracle queries. Run as SageMaker Processing Job.  
   **Inputs**: S3 raw paths (boto3 list), config thresholds (e.g., z-score filter).  
   **Outputs**: Parquet files in S3 processed layer; Athena tables created/updated.  
   **Success Criteria**: Processes 1M rows <5 min; imputation accuracy (e.g., KNN fills >95% missing); queryable via Athena SQL (joins/windows); no data loss.  
   **Mapped Libraries/Skills**: pandas (ETL, cleansing, merge/pivot), numpy (vector ops), scikit-learn (KNN imputation, outlier detection), sqlalchemy (embedded SQL for Athena sim), scipy (z-score/percentiles), joblib (parallel ETL), boto3/pyarrow (S3 Parquet read/write from 00002).

3. **ml_forecasting.py**  
   **Function**: Train/compare models on processed data (Prophet for seasonality, scikit-learn RF/GB for multivariate, optional Amazon Forecast); forecast 3-6 months; evaluate accuracy. Run as SageMaker Training Job. Covers Citi's ML forecasting/models.  
   **Inputs**: S3 processed Parquet paths; config (hyperparams, horizons).  
   **Outputs**: Model artifacts to S3; forecasts as Parquet/CSV; metrics (MAE/RMSE) logged.  
   **Success Criteria**: Forecast accuracy >85% (20-30% better than baseline); MAE <5%; models train <1 min on small data; Prophet handles seasonality (e.g., end-of-quarter peaks).  
   **Mapped Libraries/Skills**: Prophet (time-series + holidays), scikit-learn (RandomForest/GradientBoosting, clustering for features), statsmodels (ADF stationarity tests), pandas/numpy (feature eng: lags/rolling), joblib (parallel training), sagemaker/boto3 (jobs, Amazon Forecast integration from 00002).

4. **risk_analysis.py**  
   **Function**: Analyze forecasts for risks (P95 >80% flags, seasonal peaks); cluster underutilized patterns; recommend consolidations/cost savings. Covers Citi's risk analysis/optimization.  
   **Inputs**: S3 forecasts/processed Parquet.  
   **Outputs**: Risk-flagged CSV/Parquet to S3; optimization reports (e.g., 10% savings calc).  
   **Success Criteria**: Flags >90% of simulated high-risk servers; K-Means clusters >80% accuracy; recommendations reduce "costs" by 10-40% in sims.  
   **Mapped Libraries/Skills**: scikit-learn (K-Means clustering, regression for savings), scipy (P95/percentiles, stats modeling), pandas/numpy (data handling), joblib (parallel sims), boto3 (S3 I/O from 00002).

5. **visualization.py**  
   **Function**: Generate plots (trends/forecasts vs actual), dashboards, reports (PDF/Excel); optional QuickSight upload. Covers Citi's reporting/dashboards.  
   **Inputs**: S3 forecasts/risks Parquet.  
   **Outputs**: PNG/PDF/Excel reports to S3/local; Dash app for interactive (optional).  
   **Success Criteria**: Interactive plots load <2 sec; reports match specs (e.g., heatmaps, line charts); exported to QuickSight without errors.  
   **Mapped Libraries/Skills**: matplotlib/seaborn (static plots), plotly/dash (interactive dashboards), openpyxl/reportlab (Excel/PDF export), pandas (data prep), boto3 (S3/QuickSight upload from 00002).

### Optional Automation/Deployment Scripts
- **deploy_sagemaker_job.py**: Trigger ETL/ML jobs via boto3/SageMaker SDK. Covers 00002 automation.  
- **lambda_handler.py**: Serverless scheduler (EventBridge trigger) for periodic runs. Inputs: Event payload; Outputs: Job status; Success: Jobs complete without errors.

### PowerShell Script to Create Structure & Placeholders
Save this as `C:\pyproj\AWS-CapacityForecaster\scripts\setup_environment.ps1`. Run in PowerShell: `.\scripts\setup_environment.ps1`.

```powershell
# Setup AWS-CapacityForecaster directory structure and placeholders

$basePath = "C:\pyproj\AWS-CapacityForecaster"

# Create folders
New-Item -Path "$basePath\src" -ItemType Directory -Force
New-Item -Path "$basePath\src\utils" -ItemType Directory -Force
New-Item -Path "$basePath\notebooks" -ItemType Directory -Force
New-Item -Path "$basePath\tests" -ItemType Directory -Force
New-Item -Path "$basePath\config" -ItemType Directory -Force
New-Item -Path "$basePath\scripts" -ItemType Directory -Force
New-Item -Path "$basePath\docs\images" -ItemType Directory -Force  # Assuming docs exists

# Populate empty src files
New-Item -Path "$basePath\src\__init__.py" -ItemType File -Force
New-Item -Path "$basePath\src\data_generation.py" -ItemType File -Force
New-Item -Path "$basePath\src\etl_pipeline.py" -ItemType File -Force
New-Item -Path "$basePath\src\ml_forecasting.py" -ItemType File -Force
New-Item -Path "$basePath\src\risk_analysis.py" -ItemType File -Force
New-Item -Path "$basePath\src\visualization.py" -ItemType File -Force

# Utils
New-Item -Path "$basePath\src\utils\__init__.py" -ItemType File -Force
New-Item -Path "$basePath\src\utils\aws_utils.py" -ItemType File -Force
New-Item -Path "$basePath\src\utils\data_utils.py" -ItemType File -Force
New-Item -Path "$basePath\src\utils\ml_utils.py" -ItemType File -Force
New-Item -Path "$basePath\src\utils\config.py" -ItemType File -Force

# Notebooks (empty .ipynb placeholders - add JSON content manually or via Jupyter)
New-Item -Path "$basePath\notebooks\01_data_generation.ipynb" -ItemType File -Force
New-Item -Path "$basePath\notebooks\02_etl_pipeline.ipynb" -ItemType File -Force
New-Item -Path "$basePath\notebooks\03_ml_forecasting.ipynb" -ItemType File -Force
New-Item -Path "$basePath\notebooks\04_risk_analysis.ipynb" -ItemType File -Force
New-Item -Path "$basePath\notebooks\05_visualization.ipynb" -ItemType File -Force

# Tests
New-Item -Path "$basePath\tests\test_data_generation.py" -ItemType File -Force
New-Item -Path "$basePath\tests\test_etl_pipeline.py" -ItemType File -Force
New-Item -Path "$basePath\tests\test_ml_forecasting.py" -ItemType File -Force
New-Item -Path "$basePath\tests\test_risk_analysis.py" -ItemType File -Force
New-Item -Path "$basePath\tests\test_visualization.py" -ItemType File -Force

# Config
New-Item -Path "$basePath\config\config.yaml" -ItemType File -Force

# Scripts
New-Item -Path "$basePath\scripts\deploy_sagemaker_job.py" -ItemType File -Force
New-Item -Path "$basePath\scripts\lambda_handler.py" -ItemType File -Force

# Root files
New-Item -Path "$basePath\requirements.txt" -ItemType File -Force
New-Item -Path "$basePath\README.md" -ItemType File -Force
New-Item -Path "$basePath\.gitignore" -ItemType File -Force
New-Item -Path "$basePath\.env" -ItemType File -Force

Write-Output "Directory structure created and placeholders populated at $basePath"
