According to the overall planning for **AWS-CapacityForecaster** (the cloud-native, Citi-inspired enterprise capacity forecasting & optimization portfolio project), the file

**`src/utils/data_utils.py`**

is intended to be a **central, reusable module** containing helper functions focused on **data preparation, synthetic data creation, loading, basic transformation, and quality checks** — everything needed before feeding data into feature engineering, ML model training, or Athena/S3-based querying.

### Main Purposes of `src/utils/data_utils.py`

This file should **not** contain business logic, model training, visualization, or AWS-specific orchestration (those belong in notebooks, pipelines, or other modules like `src/pipeline/`, `src/models/`, `src/aws/`).

Instead, it acts as a clean, well-tested utility layer with functions that are called from multiple notebooks/scripts.
**Current implementation** includes:

1. **Synthetic Enterprise Server Metrics Generation** (highest priority early in the project)
   - Create realistic, Citi-style time-series data simulating daily/hourly P95 metrics from monitoring tools (BMC TrueSight / TSCO style)
   - Parameters: number of servers, time range, seasonality (e.g. end-of-quarter spikes), trends, noise, holidays/banking calendar effects
   - Output: pandas DataFrame with columns like `server_id`, `date`, `cpu_p95`, `memory_p95`, `disk_p95`, `network_out_p95`, `business_unit`, `criticality`, etc.

2. **Data Loading from Various Sources**
   - `load_from_s3(bucket, key)` → download CSV/Parquet from S3 using boto3 + pandas
   - `load_local_csv(path)` → for development / quick testing
   - Future: load from Athena (via awswrangler or boto3) or simulate Oracle backup DB extraction

3. **Basic Data Validation & Quality Checks**
   - `validate_capacity_df(df)` → check for expected columns, date range, negative values, extreme outliers, missing server_ids
   - `detect_anomalies_simple(df, threshold=3.0)` → basic z-score flagging

4. **Light Preprocessing / Helper Transformations** (before full feature engineering)
   - `resample_to_daily(df)` → if input is hourly
   - `add_datetime_components(df)` → extract year, month, quarter, dayofweek, is_eoq, is_holiday, etc.
   - `clean_numerical_columns(df)` → clip negatives → 0, basic missing value handling (forward-fill or interpolate short gaps)

5. **Small Utility Functions Used Across the Project**
   - `generate_server_metadata(n_servers)` → create lookup table with server_id, app_name, business_unit, criticality, region, etc.
   - `merge_metrics_with_metadata(metrics_df, metadata_df)`
   - `get_date_range(start_date, end_date, freq='D')`

### Typical (Evolving) Structure Inside the File

```python
# src/utils/data_utils.py
import pandas as pd
import numpy as np
from datetime import datetime
import boto3
from typing import Optional, Tuple

def generate_synthetic_server_metrics(
    n_servers: int = 80,
    start_date: str = "2022-01-01",
    end_date: str = "2025-12-31",
    freq: str = "D",
    seed: int = 42,
    add_seasonality: bool = True,
    add_eoq_spikes: bool = True,
    noise_level: float = 0.08,
) -> pd.DataFrame:
    """Generate realistic daily P95 server metrics simulating Citi-scale monitoring data."""
    # implementation here...
    pass


def load_from_s3(
    bucket: str,
    key: str,
    profile_name: Optional[str] = None,
    region: str = "us-east-1"
) -> pd.DataFrame:
    """Download and read CSV or Parquet from S3 into DataFrame."""
    # boto3 session + s3.get_object + pd.read_csv / read_parquet
    pass


def validate_capacity_df(df: pd.DataFrame) -> Tuple[bool, list]:
    """Run basic validation checks on capacity metrics DataFrame."""
    issues = []
    # checks for columns, dtypes, missing values > 5%, negative utilization, etc.
    return len(issues) == 0, issues


def add_calendar_features(df: pd.DataFrame) -> pd.DataFrame:
    """Add time-based features useful for capacity forecasting."""
    df = df.copy()
    df["month"] = df["date"].dt.month
    df["quarter"] = df["date"].dt.quarter
    df["is_eoq"] = df["date"].dt.is_quarter_end
    # add banking holidays if desired
    return df
```

### Where This File Fits in the Folder Structure

Following the gradual evolution of the layout we discussed:

```
AWS-CapacityForecaster/
├── src/
│   ├── utils/                  ← reusable helpers live here
│   │   ├── __init__.py
│   │   ├── data_utils.py       ← this file: data generation + loading + light cleaning
│   │   ├── aws_utils.py        ← (later) boto3 wrappers, s3/athena helpers
│   │   └── metrics_utils.py    ← (later) P95 calc, risk scoring, etc.
│   ├── data/
│   │   └── raw/                ← (gitignored) small sample files
│   ├── pipeline/               ← ETL steps, full processing flows
│   ├── models/                 ← forecasting model wrappers
│   ├── visualization/          ← plotting helpers
│   └── aws/                    ← Lambda / SageMaker entry points (later)
├── notebooks/
│   ├── 01_generate_data.ipynb
│   ├── 02_exploratory_analysis.ipynb
│   └── 03_model_training.ipynb
├── tests/
│   └── test_data_utils.py      ← pytest unit tests for these functions
├── requirements.txt
└── README.md
```

Bottom line:  
**`src/utils/data_utils.py` is the single source of truth for "how we create, load, and lightly prepare the foundational monitoring/capacity data"** — so notebooks stay clean and logic stays reusable/testable.

Would you like me to:
- Draft an initial version of this file (e.g. synthetic generation function)?
- Suggest unit tests for it?
- Show how to import and use it from a notebook?
- Or move to the next logical file/module?

Let me know what feels most helpful right now.