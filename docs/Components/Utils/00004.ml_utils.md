# Design Document: ml_utils.py

## Overview
**File Path:** C:\pyproj\AWS-CapacityForecaster\src\utils\ml_utils.py  
**GitHub Location:** https://github.com/seanlgirgis/AWS-CapacityForecaster/blob/main/src/utils/ml_utils.py  

This utility module serves as a centralized collection of machine learning (ML) functions tailored to the core objectives of the AWS-CapacityForecaster project. It prioritizes time-series forecasting for infrastructure metrics (e.g., CPU, memory, disk utilization), feature engineering, model training/comparison, and evaluation—directly aligning with the project's #1 target (Data Science & ML). Secondary support includes capacity planning elements like risk flagging and optimization recommendations (#2 target), while leveraging Python libraries for clean, efficient implementation (#3 target).  

The module is designed for modularity and reusability: functions can be imported into notebooks (e.g., SageMaker Studio) or scripts for ETL pipelines, model training, and reporting. It simulates Citi-style workflows by handling synthetic/high-volume telemetry data, incorporating seasonality (e.g., banking peaks), and enabling AWS integration (e.g., via boto3 for S3 data access in related modules).  

**Key Principles:**  
- **Modularity:** Each function is self-contained, with clear inputs/outputs for easy testing and integration.  
- **Efficiency:** Use vectorized operations (numpy/pandas) and parallelization (joblib) to handle large datasets (e.g., millions of rows).  
- **Extensibility:** Functions support multiple models (Prophet, scikit-learn) for comparison, with hooks for Amazon Forecast integration.  
- **Documentation:** Inline docstrings per PEP 8, with examples. No direct AWS calls here—defer to separate utils (e.g., data_utils.py for S3/Athena).  

## Requirements & Functionality
This module addresses the following project needs, derived from Citi experience (e.g., forecasting with Prophet/scikit-learn, feature engineering with pandas, risk analysis with scipy):  

### Functional Requirements  
1. **Feature Engineering:** Transform raw time-series data (e.g., daily P95 metrics) into model-ready features, including lags, rolling statistics, and seasonal indicators.  
2. **Model Training:** Train and fit forecasting models (Prophet for seasonality, scikit-learn for multivariate regression).  
3. **Model Evaluation & Comparison:** Compute accuracy metrics (e.g., MAE, RMSE) and compare models against baselines.  
4. **Forecasting & Prediction:** Generate future predictions with confidence intervals.  
5. **Risk Analysis & Optimization:** Flag at-risk resources (e.g., P95 exceedance) and cluster for underutilization detection.  

### Non-Functional Requirements  
- **Performance:** Process 100k+ rows in <1 minute on standard hardware (use joblib for parallelism).  
- **Scalability:** Functions accept pandas DataFrames for easy scaling to AWS SageMaker jobs.  
- **Dependencies:** Minimal—core libs (pandas, numpy, scikit-learn, prophet, scipy, statsmodels, joblib). No external APIs.  
- **Error Handling:** Robust input validation (e.g., check for datetime indices) and logging.  
- **Testing:** Each function includes example usage for unit testing (e.g., via pytest in tests/ folder).  

## Design
### Architecture  
- **Structure:** A flat Python module with functions grouped by category (e.g., preprocessing, modeling, evaluation). No classes unless needed for stateful models.  
- **Inputs/Outputs:** Primarily pandas DataFrames/Series for data, dicts for configs/hyperparams. Outputs include fitted models, predictions (DataFrames), and metrics (dicts).  
- **Integration Points:**  
  - Import from data_utils.py for cleansed data.  
  - Export models/predictions to S3 via boto3 (handled in main scripts).  
  - Use in SageMaker notebooks for training (e.g., script mode).  

### Key Functions  
Here's a high-level outline of planned functions, with signatures and purposes:  

1. **Preprocessing/Feature Engineering**  
   - `engineer_features(df: pd.DataFrame, metrics: list[str], lag_periods: int = 7, rolling_windows: list[int] = [7, 30]) -> pd.DataFrame`  
     Purpose: Add lags, rolling means/std, seasonal dummies (e.g., quarter-end flags). Mirrors Citi feature engineering for ML inputs.  
     Example: Input df with 'timestamp', 'cpu_p95'; output adds 'cpu_lag7', 'cpu_rolling_mean30'.  

   - `handle_missing_data(df: pd.DataFrame, method: str = 'interpolate', **kwargs) -> pd.DataFrame`  
     Purpose: Cleanse data (impute missing, remove outliers via z-score). Uses pandas.interpolate or scikit-learn KNN. Citi-style quality assurance.  

   - `check_stationarity(ts: pd.Series) -> dict`  
     Purpose: Run ADF test (statsmodels) for time-series stationarity; suggest differencing if needed.  

2. **Model Training & Forecasting**  
   - `train_prophet_model(df: pd.DataFrame, target: str = 'cpu_p95', regressors: list[str] = None) -> Prophet`  
     Purpose: Fit Prophet model with seasonality/holidays. Add regressors for multivariate (e.g., memory correlation). Returns fitted model.  

   - `train_sklearn_model(df: pd.DataFrame, target: str, features: list[str], model_type: str = 'random_forest', params: dict = None) -> BaseEstimator`  
     Purpose: Train scikit-learn models (RandomForestRegressor, GradientBoostingRegressor). Hyperparam tuning via params dict.  

   - `generate_forecast(model: Union[Prophet, BaseEstimator], future_periods: int, future_df: pd.DataFrame = None) -> pd.DataFrame`  
     Purpose: Predict future values with intervals. For Prophet: use make_future_dataframe; for scikit-learn: predict on engineered future features.  

3. **Evaluation & Comparison**  
   - `evaluate_model(y_true: pd.Series, y_pred: pd.Series) -> dict[str, float]`  
     Purpose: Compute metrics (MAE, RMSE, MAPE). Citi-like accuracy claims (e.g., "20-30% improvement").  

   - `compare_models(results: list[dict]) -> pd.DataFrame`  
     Purpose: Tabulate metrics across models (baseline, Prophet, ensemble) for selection.  

4. **Capacity-Specific Utilities**  
   - `flag_risks(df: pd.DataFrame, forecast_df: pd.DataFrame, threshold: float = 90.0, percentile: float = 95.0) -> pd.DataFrame`  
     Purpose: Analyze P95 risks, flag servers nearing capacity. Uses scipy.percentile for seasonal peaks.  

   - `cluster_utilization(df: pd.DataFrame, n_clusters: int = 3, features: list[str] = ['cpu_mean', 'mem_mean']) -> pd.DataFrame`  
     Purpose: K-Means clustering (scikit-learn) for underutilized groups; recommend consolidations/cost savings.  

### Dependencies  
- Import statements at top: `import pandas as pd`, `import numpy as np`, `from prophet import Prophet`, `from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor`, `from sklearn.cluster import KMeans`, `from sklearn.metrics import mean_absolute_error, mean_squared_error`, `from statsmodels.tsa.stattools import adfuller`, `from scipy import stats`, `import joblib`.  
- No AWS-specific imports—keep pure ML.  

## Implementation Plan  
- **Coding Standards:** PEP 8, type hints, docstrings with examples. Use logging for debug info.  
- **Development Steps:**  
  1. Start with feature engineering functions (core to Citi pipelines).  
  2. Add model training (Prophet first, then scikit-learn).  
  3. Implement evaluation and risk utils.  
  4. Test with synthetic data from data_generation.py.  
- **Version Control:** Commit incrementally to GitHub (e.g., "feat: add feature engineering", "feat: add Prophet training").  

## Testing & QA  
- **Unit Tests:** In tests/test_ml_utils.py—use pytest to cover edge cases (e.g., small df, missing data).  
- **Integration:** Validate in a SageMaker notebook: load data from S3, engineer features, train, forecast, evaluate.  
- **Traceability:** Maps to project SRS (if drafted): e.g., "ML Forecasting" requirement → train/evaluate functions; "Risk Analysis" → flag_risks.  

## Deployment & Maintenance  
- **Usage in Project:** Import into main pipeline scripts (e.g., src/pipeline/train_forecast.py) or notebooks.  
- **Maintenance:** Update for new models (e.g., add Amazon Forecast wrapper if integrated). Review post-project for optimizations.  
- **Lessons Learned Hook:** After implementation, note performance benchmarks (e.g., "joblib sped up clustering 3x").  

This design keeps ml_utils.py lean (~300-500 LOC) while powering the project's ML backbone. If you'd like, I can provide a code skeleton, sample function implementation, or help draft the SRS template referenced in the SDLC doc.