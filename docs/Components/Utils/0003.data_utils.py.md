### Purpose of aws_utils.py

The `aws_utils.py` script serves as a centralized utility module for handling AWS interactions within the AWS-CapacityForecaster project. Its primary purpose is to abstract away repetitive AWS operations using the `boto3` library, promoting clean, reusable code that aligns with the project's third priority target (Clean Python + AWS/cloud integration). By encapsulating AWS-specific logic here, we avoid cluttering core data processing or ML scripts (e.g., in `src/data/` or `src/models/`) with cloud details, making the codebase more maintainable and testable. This mirrors enterprise practices from your Citi experience, where modular scripts handled data ingestion from monitoring tools or databases.

Key benefits:
- **Modularity**: Functions can be imported and called from notebooks or other scripts (e.g., for uploading synthetic data to S3 during ETL).
- **Error Handling & Best Practices**: Includes logging, retries for transient AWS errors, and credential management to ensure reliability in a cloud environment.
- **Cost Awareness**: Functions are designed to minimize AWS usage (e.g., short-lived sessions, small data transfers) to stay under your $200 credits budget.
- **Extensibility**: Easy to add methods for new services like Athena or SageMaker as the project evolves, supporting the ML forecasting and capacity optimization workflows.

This script focuses on S3 and basic session management initially, as these are the foundational AWS services in our scope (data storage and ETL). It can expand to include SageMaker or Athena if needed for model training or querying.

### Design Overview

The script follows a functional design pattern, with standalone functions grouped by AWS service (e.g., S3-related functions together). It uses:
- **Imports**: `boto3` for AWS SDK, `botocore` for error handling, `logging` for traceability (echoing your Citi use of `logging`), and `os`/`pathlib` for local file handling.
- **Configuration**: Relies on environment variables or AWS CLI credentials for authentication (no hard-coded keys). A global session can be created for reuse.
- **Error Management**: Each function wraps operations in try-except blocks, raising custom exceptions or logging warnings for issues like bucket not found or upload failures.
- **Typing & Documentation**: Uses type hints (e.g., `def upload_to_s3(...) -> str`) and docstrings for clarity, adhering to PEP 8 standards.
- **Testing Considerations**: Functions are pure where possible (input/output only), making them unit-testable (e.g., with `pytest` in a future `tests/` folder).
- **Project Integration**: Located in `src/utils/` to follow the evolving folder structure (utils for shared helpers), importable as `from utils.aws_utils import upload_to_s3`.

No classes are used unless complexity grows (e.g., for a full AWS client wrapper), keeping it lightweight for a portfolio project.

### Methods Contained

Here’s a breakdown of the key methods, based on the project's needs (e.g., S3 for data storage, as outlined in the planning docs). This is a suggested implementation— we can refine or add based on your progress.

1. **get_aws_session(region: str = None) -> boto3.Session**
   - **Purpose**: Creates and returns a reusable boto3 session. Integrates with `src.utils.config` to automatically pull default region and profile settings.
   - **Parameters**:
     - `region`: AWS region (optional). If not provided, fetches `aws.region` from Config.
   - **Returns**: A `boto3.Session` object.
   - **Implementation Notes**: Uses `boto3.Session()` with optional profile support. Logs session creation for debugging.
   - **Usage Example**: `session = get_aws_session(); s3_client = session.client('s3')`

2. **create_s3_bucket(bucket_name: str, region: str = 'us-east-1') -> bool**
   - **Purpose**: Creates an S3 bucket if it doesn't exist, simulating the "Create one S3 bucket" step from the getting-started plan. Supports capacity data storage (e.g., `sean-capacity-forecast-data`).
   - **Parameters**:
     - `bucket_name`: Name of the bucket (must be globally unique).
     - `region`: AWS region for the bucket.
   - **Returns**: `True` if created or already exists; raises exception on failure.
   - **Implementation Notes**: Uses `s3_client.create_bucket()` with location constraint. Checks existence first via `head_bucket()` to avoid errors. Logs the outcome.
   - **Usage Example**: For initial setup in a notebook: `create_s3_bucket('my-forecast-bucket')`

3. **upload_to_s3(local_path: str, bucket_name: str, s3_key: str, session: boto3.Session = None) -> str**
   - **Purpose**: Uploads a local file (e.g., CSV of synthetic metrics) to S3, central to the data generation & storage phase. Enables ETL by moving cleaned data to the cloud for SageMaker or Athena access.
   - **Parameters**:
     - `local_path`: Full path to the local file (e.g., 'data/synthetic_metrics.csv').
     - `bucket_name`: Target S3 bucket.
     - `s3_key`: S3 object key (e.g., 'raw/data.csv').
     - `session`: Optional boto3 session (defaults to creating one via `get_aws_session()`).
   - **Returns**: The S3 URI (e.g., 's3://bucket/raw/data.csv') for reference in ML jobs.
   - **Implementation Notes**: Uses `s3_client.upload_file()`. Handles large files with multipart uploads if needed. Includes retry logic via `botocore` for network issues. Logs upload size and time for performance tracking.
   - **Usage Example**: After generating data: `uri = upload_to_s3('path/to/file.csv', 'my-bucket', 'data/file.csv')`

4. **download_from_s3(bucket_name: str, s3_key: str, local_path: str, session: boto3.Session = None) -> bool**
   - **Purpose**: Downloads an S3 object to local storage, useful for pulling processed data back for local visualization or further analysis (e.g., after a SageMaker job).
   - **Parameters**:
     - `bucket_name`: Source S3 bucket.
     - `s3_key`: S3 object key.
     - `local_path`: Destination local path.
     - `session`: Optional boto3 session.
   - **Returns**: `True` on success; raises exception on failure.
   - **Implementation Notes**: Uses `s3_client.download_file()`. Overwrites existing files. Logs download details.
   - **Usage Example**: `download_from_s3('my-bucket', 'processed/forecasts.csv', 'local/forecasts.csv')`

5. **list_s3_objects(bucket_name: str, prefix: str = '', session: boto3.Session = None) -> List[str]**
   - **Purpose**: Lists objects in an S3 bucket (with optional prefix), helpful for debugging or inventorying data files during pipeline runs (e.g., checking uploaded metrics).
   - **Parameters**:
     - `bucket_name`: S3 bucket.
     - `prefix`: Optional folder/key prefix (e.g., 'raw/').
     - `session`: Optional boto3 session.
   - **Returns**: List of S3 keys (strings).
   - **Implementation Notes**: Uses `s3_client.list_objects_v2()` with pagination handling for large buckets. Returns only keys, excluding folders.
   - **Usage Example**: `objects = list_s3_objects('my-bucket', 'data/')`

### Next Steps & Recommendations
- **Implementation**: If this script doesn't exist yet, start by creating it in `C:\pyproj\AWS-CapacityForecaster\src\utils\aws_utils.py` with the above skeleton. Add `if __name__ == "__main__":` for quick testing (e.g., upload a sample file).
- **Expansion Ideas**: If we integrate Athena, add `run_athena_query(query: str, database: str) -> pd.DataFrame` using `boto3.client('athena')` to simulate Oracle queries. For SageMaker, a `start_sagemaker_job(...)` method could handle training.
- **Testing**: Write unit tests in `tests/test_aws_utils.py` using `moto` (AWS mock library) to avoid real costs.
- **Commit to GitHub**: After adding, push to https://github.com/seanlgirgis/AWS-CapacityForecaster with a commit like "Add aws_utils for S3 operations".

If you'd like sample code for any method, help implementing it, or adding more (e.g., for Amazon Forecast integration), just let me know! This keeps us laser-focused on Python + AWS while building toward ML forecasting.