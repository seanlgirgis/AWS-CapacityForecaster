The **very next piece** to implement — the highest-leverage single addition right now — is:

**`src/pipeline.py`** (or alternatively named `src/main_pipeline.py` / `src/orchestrator.py`)

**Why this is the clear #1 next step (as of Jan 25, 2026 repo state):**
- The project already has strong modular pieces: excellent config system (`src/utils/config.py`), mature data generation & ETL utilities (`src/utils/data_utils.py`), notebooks covering phases 01–05, recent addition of reports/ artifacts + an S3 push script in `scripts/`.
- What is still blocking "end-to-end usability" and true enterprise-grade feel is **a single, runnable script** that ties everything together outside of notebooks.
- Notebooks are great for exploration and portfolio proof-of-concept, but recruiters / reviewers want to see: "clone → install → run one command → get forecasts + risks + reports in S3".
- This component directly advances **all three core targets**:
  1. ML: triggers training / forecasting / evaluation
  2. Capacity Planning: executes risk flagging + clustering + savings calc
  3. AWS/Cloud: handles S3 load/save, can later become Lambda entry point or SageMaker Processing script
- Recent commit added reports/ + S3 push → the pipeline can now realistically produce and persist tangible outputs.

**Target functionality of `src/pipeline.py` (what it should achieve when complete):**

- Serve as **the primary orchestrator / CLI entry point** for the entire system.
- Accept simple command-line control so you (or a future user) can run different modes without opening Jupyter.
- Load and validate the configuration (using existing `config.py`).
- Execute the full logical flow in sequence (or selected parts):
  1. (Optional) Generate fresh synthetic data → save raw to S3/local
  2. Load data (from S3 preferred, fallback to local)
  3. Run full ETL + feature engineering + cleaning (call functions from `data_utils.py`)
  4. (If training mode) Train Prophet + RandomForest + GradientBoosting models → evaluate (MAE/RMSE/MAPE + cross-val) → save models & metrics to S3 / models/
  5. Generate forecasts (using latest or loaded models) for next 90–180 days
  6. Perform risk analysis: P95 thresholds, seasonal/EOQ peak flags, at-risk server list by BU/region/criticality
  7. Run utilization clustering (K-Means n=4) → generate consolidation recommendations + rough cost-savings estimate
  8. Create key visualizations (plotly interactive + static matplotlib/seaborn) → save to reports/
  9. Generate and export final reports:
     - PDF executive summary (using reportlab or matplotlib)
     - Excel with detailed forecasts/risks/clusters (openpyxl)
     - JSON summary (for potential Lambda return or downstream)
  10. Upload all artifacts (forecast CSVs, models, reports, metrics) to configured S3 prefixes
- Include structured logging (logging module + timestamps + levels)
- Handle basic errors gracefully (e.g. missing S3 data → fallback, config validation failures)
- Support modes via argparse (examples):
  ```bash
  python src/pipeline.py --mode full --env local
  python src/pipeline.py --mode forecast-only --use-saved-model
  python src/pipeline.py --mode generate-data --upload-s3
  python src/pipeline.py --help
  ```
- Be **Lambda-friendly in structure** (e.g. main logic inside a function that can later be called from `lambda_handler(event, context)`)

**Rough skeleton you can start with right now** (place in `C:\pyproj\AWS-CapacityForecaster\src\pipeline.py`):

```python
import argparse
import logging
from pathlib import Path

from utils.config import load_config, validate_config
from utils.data_utils import (
    generate_synthetic_data,
    load_data_from_s3_or_local,
    run_etl_and_feature_engineering,
    save_processed_data
)
# Import your model training, forecasting, risk, clustering, reporting functions here
# e.g. from models.forecasting import train_and_evaluate, generate_forecasts
# from analysis.risk import run_risk_analysis
# from analysis.clustering import run_clustering
# from reporting.generate_report import create_pdf_report, export_to_excel

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def run_full_pipeline(config):
    logger.info("Starting full pipeline...")
    
    # 1. Data
    if config.get('generate_data', False):
        df_raw = generate_synthetic_data(config)
        save_processed_data(df_raw, config, prefix="raw")
    else:
        df_raw = load_data_from_s3_or_local(config)
    
    # 2. ETL + Features
    df_processed = run_etl_and_feature_engineering(df_raw, config)
    save_processed_data(df_processed, config, prefix="processed")
    
    # 3. Forecasting / ML (placeholder)
    # metrics = train_and_evaluate(df_processed, config)
    # forecasts = generate_forecasts(df_processed, config)
    
    # 4. Risk & Clustering (placeholder)
    # risk_results = run_risk_analysis(forecasts, config)
    # cluster_results = run_clustering(df_processed, config)
    
    # 5. Reporting & Export (placeholder)
    # create_pdf_report(risk_results, cluster_results, forecasts, config)
    # export_to_excel(..., config)
    
    logger.info("Pipeline completed successfully.")

def main():
    parser = argparse.ArgumentParser(description="AWS-CapacityForecaster Pipeline")
    parser.add_argument("--mode", choices=["full", "generate-data", "forecast-only", "risk-only"], default="full")
    parser.add_argument("--env", choices=["local", "sagemaker", "lambda"], default="local")
    parser.add_argument("--config-path", type=str, default="config/config.yaml")
    args = parser.parse_args()

    config = load_config(args.config_path, env=args.env)
    validate_config(config)

    if args.mode == "full":
        run_full_pipeline(config)
    elif args.mode == "generate-data":
        # ... call only data gen + save
        pass
    # Add other modes...

if __name__ == "__main__":
    main()
```

**Next immediate actions you can take today:**
1. Create the file at `C:\pyproj\AWS-CapacityForecaster\src\pipeline.py`
2. Copy-paste the skeleton above
3. Start filling in the real function calls from your existing utils / notebooks (start with data load → ETL → save)
4. Test locally: `python src/pipeline.py --mode full --env local`
5. Commit & push → this becomes your new "run me" entry point

Once this exists and runs even a partial flow (e.g. data → processed → saved to S3), the project instantly feels much more complete and production-like.

Want me to expand any specific section of this skeleton (e.g. add argparse modes, integrate reporting, add model calling logic)? Or prefer to jump to the second-highest priority (model save/load) next?