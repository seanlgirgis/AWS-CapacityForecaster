Here is the **complete Markdown design document** for `module_02_data_load.py`, written in the style consistent with your project's documentation approach (e.g., detailed purpose, In/Process/Out structure, pseudo-logic, manual verification steps, and clear hand-off to the next developer).

You can copy-paste this directly into:

**`C:\pyproj\AWS-CapacityForecaster\my_Docs\00013.pipeline.modules\module_02_data_load.py.md`**

(or create/commit it to the repo at the same path).

```markdown
# Module Design Document: module_02_data_load.py

**File Path**  
`src/modules/module_02_data_load.py`

**Module Sequence Number**  
02 (immediately follows module_01_data_generation.py)

**Last Updated**  
January 2026

**Author / Maintainer**  
seanlgirgis (with Grok assistance)

## 1. Purpose & Business Context

This module is the **safe, auditable ingestion gateway** for all downstream processing in the AWS-CapacityForecaster pipeline.

It takes the raw synthetic (or future real) server metrics Parquet file generated by module_01 and:

- Loads it reliably (S3 preferred, local fallback)
- Performs strict schema, dtype, and quality validation
- Produces logging + JSON summary for traceability
- Optionally saves a "validated/staged" copy
- Guarantees a clean, sorted, trusted DataFrame for module_03 (ETL & feature engineering)

**Citi Realism Alignment**  
In real enterprise monitoring (BMC TrueSight, AppDynamics, CA Wily), ingestion feeds often arrive with:
- Small percentage of missing values
- Occasional date gaps from agent failures
- Inconsistent timestamp formats
- Unexpected columns or dtypes

This module simulates that defensive loading layer — failing fast on critical issues while warning on recoverable ones.

**Pipeline Hand-off Guarantee**  
- Output: pandas DataFrame with exact schema
- Side effects: staged Parquet + JSON summary
- Next module (03): can assume data is clean, sorted, datetime-typed, and validated

## 2. In / Process / Out Structure

### In (Inputs – Strictly Config-Driven)

- **Configuration object** (from `utils.config.load_config`)
  - `storage.use_s3`: bool → prefer S3 load
  - `storage.bucket_name`
  - `storage.raw_prefix`: e.g. "raw/"
  - `storage.local_base_path`: e.g. "data/scratch"
  - `storage.raw_filename`: optional specific file (fallback: latest in prefix)
  - `data.num_servers`: expected server count for validation
  - `validation.max_missing_rate`: float (e.g. 5.0) → % threshold per numeric column
- **File artifact from module_01**
  - Parquet file matching pattern: `raw_server_metrics_YYYYMMDD_to_YYYYMMDD.parquet`
  - Location: S3 `{bucket}/{raw_prefix}/...` or local `{local_base_path}/raw/...`

### Process (Core Logic – Pseudo-Code Style)

```text
1. Load config & validate required keys
2. Determine target file path:
   if specific filename in config → use it
   else → discover latest .parquet in raw_prefix (S3 list_objects or local glob + mtime sort)
3. Load Parquet → pandas.read_parquet (via boto3 wrapper or local)
   if load fails → raise FileNotFoundError with clear message
4. Schema validation:
   required_columns = ['server_id', 'timestamp', 'business_unit', 'region', 'criticality',
                       'cpu_p95', 'mem_p95', 'disk_p95', 'net_in_p95', 'net_out_p95']
   if any missing → raise ValueError
5. Dtype fixes:
   df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')
   if any NaT after coercion → log warning
6. Quality gates:
   - server count == config.num_servers ? warn if mismatch
   - date continuity per server: groupby server_id → diff(days) >1 ? count & warn
   - missing rate per numeric column ≤ validation.max_missing_rate ? raise if exceeded
7. Final preparation:
   sort by ['server_id', 'timestamp']
   reset_index(drop=True)
8. Create summary dict (rows, servers, date_range, missing_rates, validation_passed)
9. Save:
   - staged Parquet (optional, prefix="staged/")
   - JSON summary (prefix="reports/summaries/")
10. Log detailed success message with paths
```

### Out (Outputs – Predictable & Auditable)

- **Primary return value** (for pipeline chaining):  
  `pd.DataFrame` — validated, sorted, datetime-ready, schema-guaranteed

- **Side artifacts** (saved via `utils.data_utils.save_processed_data`):
  - Staged Parquet: `validated_server_metrics_YYYYMMDD_to_YYYYMMDD.parquet`  
    Prefix: `staged/` (local or S3)
  - JSON summary: `module_02_summary.json`  
    Prefix: `reports/summaries/`
    Content example:
    ```json
    {
      "module": "02_data_load",
      "loaded_at": "2026-01-25T16:48:00Z",
      "rows": 175320,
      "unique_servers": 120,
      "date_range": "2022-01-01 00:00:00 → 2025-12-31 00:00:00",
      "missing_rates": {
        "cpu_p95": 0.970,
        "mem_p95": 1.036,
        ...
      },
      "validation_passed": true
    }
    ```

## 3. How to Run Manual Checks (Post-Execution Verification)

After running the module, confirm success with these commands (from project root):

```powershell
# 1. Quick shape, servers, date range, missing %
python -c "
import pandas as pd
df = pd.read_parquet('data/scratch/staged/validated_server_metrics_20220101_to_20251231.parquet')
print('Shape:', df.shape)
print('Unique servers:', df['server_id'].nunique())
print('Date range:', df['timestamp'].min(), '→', df['timestamp'].max())
print('\nMissing %:\n', (df.isna().mean() * 100).round(3))
print('\nHead:\n', df.head())
"

# 2. Check for date continuity gaps per server
python -c "
import pandas as pd
df = pd.read_parquet('data/scratch/staged/validated_server_metrics_20220101_to_20251231.parquet')
gaps = df.groupby('server_id')['timestamp'].apply(lambda x: (x.sort_values().diff().dt.days > 1).sum())
print('Servers with date gaps:', (gaps > 0).sum())
if (gaps > 0).any():
    print(gaps[gaps > 0])
else:
    print('No date gaps detected — good continuity')
"

# 3. Inspect JSON summary
type data\scratch\reports\summaries\module_02_summary.json
# or open in VS Code / Notepad
```

**Expected Success Signs**
- Shape matches module_01 output
- Unique servers = config num_servers (120)
- Missing rates ≈1% (from generation)
- No or very few date gaps (synthetic data should be continuous)
- JSON shows `"validation_passed": true`

**Failure Signs & Actions**
- Missing columns → check module_01 output
- High missing % → adjust validation threshold or inspect generation
- Date gaps → bug in timestamp generation (unlikely in synthetic)

## 4. Next Module Hand-off

**module_03_etl_feature_eng.py** can safely assume:
- DataFrame is sorted by server_id + timestamp
- timestamp is datetime64
- All expected columns present with correct dtypes
- Missing values exist only in numeric metrics (≤ configured threshold)
- No duplicate rows or major continuity breaks

Ready for feature engineering (lags, rolling stats, calendar flags, EOQ/holiday dummies, etc.).

---
**End of Design Document**
```