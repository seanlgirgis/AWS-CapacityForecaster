
# Data Schema & S3 Organization - AWS-CapacityForecaster

**Version**: 1.0  
**Date**: January 17, 2026  
**Author**: Sean Girgis  

## Purpose
This document defines the data schema, file formats, partitioning strategy, and S3 folder structure for **AWS-CapacityForecaster**.  

It ensures:
- Efficient storage & querying with **Amazon Athena** (pay-per-query, low cost for small datasets)
- Optimal performance for time-series ML models (Prophet, scikit-learn)
- Simulation of Citi-style enterprise monitoring data (daily P95 metrics from thousands of endpoints)
- Scalability & cost control (Parquet columnar format + date-based partitioning)

## Overall Data Layers (Medallion Architecture)
We use a simple 2-layer data lake pattern (common in AWS best practices):

1. **Raw Layer** (bronze) — Immutable landing zone for synthetic/generated data
2. **Processed Layer** (silver) — Cleaned, feature-engineered, Parquet-optimized for ML & Athena

(No gold/curated layer yet — can add later for final dashboards/aggregated views)

## S3 Bucket & Folder Structure
Bucket name: `sean-capacity-forecast-data` (create in AWS console or boto3)

```
s3://sean-capacity-forecast-data/
├── raw/                       # Immutable synthetic CSVs (landing zone)
│   ├── server_metrics/        # Main dataset
│   │   ├── year=2022/
│   │   │   ├── month=01/
│   │   │   │   ├── day=01/
│   │   │   │   │   ├── server_id=server001.csv
│   │   │   │   │   ├── server_id=server002.csv
│   │   │   │   │   └── ...
│   │   │   │   └── ...
│   │   │   └── ...
│   │   └── year=2025/
│   └── metadata/              # Optional: generation logs, configs
│
└── processed/                 # Parquet, partitioned, cleaned + features
    ├── server_metrics/
    │   ├── year=2022/
    │   │   ├── month=01/
    │   │   │   ├── day=01/
    │   │   │   │   └── data.parquet  (or small files per server_id if needed)
    │   │   │   └── ...
    │   └── year=2025/
    └── features/              # Enriched for ML (lags, rolling, external flags)
        ├── year=2022/
        │   ├── month=01/
        │   │   └── data.parquet
        └── ...
```

**Partitioning Keys** (Hive-style for Athena):
- `year=YYYY` (int)
- `month=MM` (int, padded)
- `day=DD` (int, padded)

This enables efficient Athena queries like `WHERE year=2024 AND month=12` (prunes 99% of data for recent forecasts). For 100 servers × 3+ years daily (~110k rows), partitions keep file counts reasonable and scans cheap.

## Raw Data Format (CSV)
- **Format**: CSV (header row, comma-separated)
- **Why CSV for raw?** Easy to generate locally with pandas, human-readable for debugging, mimics exported monitoring tool dumps (e.g., BMC TrueSight CSV exports).
- **Filename convention**: `server_id=XXXXXX.csv` (e.g., `server_id=webapp-prod-001.csv`)

**Schema (Raw CSV columns)**:

| Column Name       | Data Type | Description                                                                 | Example Value          | Nullable? |
|-------------------|-----------|-----------------------------------------------------------------------------|------------------------|-----------|
| timestamp         | string    | ISO 8601 date-time (UTC) - daily granularity                                | 2024-12-31T00:00:00Z   | No        |
| server_id         | string    | Unique server/endpoint identifier                                           | webapp-prod-001        | No        |
| cpu_p95           | float     | 95th percentile CPU utilization (%) over the day                            | 72.45                  | No        |
| memory_p95        | float     | 95th percentile memory utilization (%)                                      | 68.90                  | No        |
| disk_p95          | float     | 95th percentile disk utilization (%) or I/O wait time                       | 45.20                  | Yes       |
| network_in_p95    | float     | 95th percentile inbound network throughput (MB/s)                           | 120.5                  | Yes       |
| network_out_p95   | float     | 95th percentile outbound network throughput (MB/s)                          | 85.3                   | Yes       |
| business_critical | int       | 1 = high criticality (e.g., trading systems), 0 = standard                  | 1                      | No        |
| is_end_of_quarter | bool      | External regressor flag for banking seasonal peaks                          | true                   | No        |

**Notes**:
- Daily aggregation (P95) simulates real monitoring tools (e.g., TrueSight/BMC, AppDynamics).
- ~100-500 rows per server CSV (3-5 years daily).
- Total raw size: < 50 MB — pennies on S3.

## Processed Data Format (Parquet)
- **Format**: Apache Parquet (columnar, compressed with Snappy)
- **Why Parquet?** AWS best practice for Athena (10-100× faster queries, lower scan cost vs CSV), great for ML (pandas.read_parquet fast, preserves types).
- **Partitioning**: Same as raw (year/month/day) — enables partition pruning in Athena/SageMaker.

**Schema (Processed Parquet)** — same as raw + added features:

| Column Name              | Data Type | Description (new/added)                                   | Notes                          |
|--------------------------|-----------|-----------------------------------------------------------|--------------------------------|
| timestamp                | timestamp | Converted to proper timestamp type                        | For time-series indexing       |
| server_id                | string    | —                                                         | Partition key candidate        |
| cpu_p95                  | float     | —                                                         | —                              |
| memory_p95               | float     | —                                                         | —                              |
| disk_p95                 | float     | —                                                         | —                              |
| network_in_p95           | float     | —                                                         | —                              |
| network_out_p95          | float     | —                                                         | —                              |
| business_critical        | int       | —                                                         | —                              |
| is_end_of_quarter        | boolean   | —                                                         | External regressor             |
| cpu_lag7                 | float     | CPU P95 lagged by 7 days (weekly seasonality)             | Feature engineering            |
| cpu_rolling_mean_30      | float     | 30-day rolling mean of cpu_p95                            | Trend detection                |
| cpu_p95_anomaly_score    | float     | Simple z-score or isolation forest score (optional)       | For outlier flagging           |
| risk_flag                | int       | 1 = high risk (P95 > 80% during peaks), 0 otherwise       | Capacity planning output       |

**Parquet specifics**:
- Compression: Snappy (fast read, good ratio)
- Row group size: Default (128MB) — fine for our scale
- Write with pandas.to_parquet(engine='pyarrow', partition_cols=['year','month','day'])

## Athena Table Definitions (DDL)
Create these tables in Athena after crawler or manual DDL for querying:

**Raw Table Example** (CSV):
```sql
CREATE EXTERNAL TABLE raw_server_metrics (
  timestamp STRING,
  server_id STRING,
  cpu_p95 FLOAT,
  memory_p95 FLOAT,
  ...
)
PARTITIONED BY (year INT, month INT, day INT)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
LOCATION 's3://sean-capacity-forecast-data/raw/server_metrics/'
TBLPROPERTIES ('skip.header.line.count'='1');
```

**Processed Table Example** (Parquet):
```sql
CREATE EXTERNAL TABLE processed_server_metrics (
  timestamp TIMESTAMP,
  server_id STRING,
  cpu_p95 FLOAT,
  ...
)
PARTITIONED BY (year INT, month INT, day INT)
STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat'
OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'
LOCATION 's3://sean-capacity-forecast-data/processed/server_metrics/';
```

Run `MSCK REPAIR TABLE processed_server_metrics;` after adding partitions.

## Design Decisions & Trade-offs
- **Daily granularity** — Matches capacity planning reports (monthly/seasonal); sufficient for 3-6 month forecasts.
- **Parquet + partitioning** — Reduces Athena query cost to pennies; enables fast ML data loading in SageMaker.
- **Synthetic only** — No real sensitive data; easy to regenerate.
- **Extensible** — Add hourly later if needed (more partitions).

