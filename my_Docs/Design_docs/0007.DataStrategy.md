# Data Strategy for AWS-CapacityForecaster

**Document ID:** 0007
**Version:** 1.0
**Date:** 2026-01-22
**Author:** Sean L. Girgis
**Status:** Draft → Review → Approved

---

## Executive Summary

This document outlines the complete data strategy for the AWS-CapacityForecaster project, combining synthetic data generation with public dataset integration to create a robust, enterprise-grade capacity forecasting system. The strategy balances practical development needs (custom synthetic data) with real-world validation (public cluster traces) to demonstrate both ML engineering skills and industry-standard practices.

**Key Decision:** Hybrid approach using 80% synthetic data (primary) + 20% real public data (validation)

---

## Table of Contents

1. [Data Requirements](#1-data-requirements)
2. [Public Dataset Landscape Analysis](#2-public-dataset-landscape-analysis)
3. [Selected Datasets](#3-selected-datasets)
4. [Synthetic Data Strategy](#4-synthetic-data-strategy)
5. [Data Integration Plan](#5-data-integration-plan)
6. [Data Quality Framework](#6-data-quality-framework)
7. [Implementation Roadmap](#7-implementation-roadmap)
8. [Success Metrics](#8-success-metrics)

---

## 1. Data Requirements

### 1.1 Project Requirements (from config.yaml)

**Current Configuration:**
- **Servers:** 120 instances
- **Duration:** 4 years (2022-01-01 to 2025-12-31)
- **Granularity:** Daily (with hourly future option)
- **Metrics:** CPU P95, Memory P95, Disk P95, Network In/Out

**Seasonality Patterns:**
- Weekly patterns (business hours vs. off-hours)
- Quarterly peaks (end-of-quarter banking surges)
- Holiday effects (reduced load during holidays)

**P95 Ranges:**
- CPU: 10-95%
- Memory: 15-92%
- Disk: 5-85%
- Network In: 20-500 Mbps
- Network Out: 10-300 Mbps

### 1.2 ML Requirements

**For Time-Series Forecasting:**
- Minimum 90 days (3 months) historical data
- Ideal: 365+ days (1+ years) for seasonal pattern learning
- Multiple servers for clustering analysis (50-200 range)
- Labeled anomalies/incidents (optional enhancement)

**For Risk Analysis:**
- High utilization events (>80% P95)
- Underutilization patterns (<40% average)
- Seasonal vulnerability periods (quarter-end, month-end)

**For Model Evaluation:**
- Test split: 20% (most recent data)
- Cross-validation: 5 folds
- Required metrics: MAE, RMSE, MAPE

### 1.3 Business Requirements

**Enterprise Realism:**
- Banking/financial services workload patterns
- Citi-inspired capacity planning workflows
- Production-grade data quality (>95% completeness)
- Realistic infrastructure heterogeneity

**Portfolio Presentation:**
- Visually compelling visualizations
- Clear business value demonstration
- Real-world data validation capability
- Industry-standard practices

---

## 2. Public Dataset Landscape Analysis

### 2.1 Research Summary

**Datasets Evaluated:** 15+ public infrastructure datasets
**Search Scope:** Academic repositories, cloud providers, research institutions
**Evaluation Period:** January 2026

### 2.2 Top-Tier Datasets

#### A. Google Cluster Data (Borg Traces)

**Source:** https://github.com/google/cluster-data

**Versions Available:**
- ClusterData 2011-2: 29 days, ~12.5K machines, 41GB compressed
- ClusterData 2019: May 2019, 8 Borg cells, 2.4TB compressed (BigQuery only)

**Metrics Included:**
- CPU usage histograms (5-minute intervals)
- Memory usage
- Task/job events and resource requests
- Machine attributes and constraints
- Alloc sets (shared resource reservations)

**Access:**
- Free under CC-BY license
- Available via Google Cloud Storage and BigQuery
- CSV format with 6 principal tables

**Strengths:**
- Production workloads from Google's infrastructure
- Comprehensive task scheduling data
- Well-documented schema
- Widely cited in academic research (1000+ papers)

**Challenges:**
- Very large size (2.4TB for 2019)
- BigQuery-only access for latest version
- Different granularity (5-min vs. daily/hourly)
- Task-centric (not server-centric like our use case)

**Compatibility Score:** 7/10 (High quality, but requires transformation)

---

#### B. Alibaba Cluster Trace Data

**Source:** https://github.com/alibaba/clusterdata

**Versions Available:**
- cluster-trace-v2018: 8 days, ~4K machines, 48GB compressed (280GB extracted)
- cluster-trace-gpu-v2020: 2 months, 6.5K GPUs, ~1.8K machines
- cluster-trace-microservices-v2021: 12 hours, 20K+ microservices

**Metrics Included:**
- CPU/memory/disk utilization
- Online services (latency-sensitive)
- Batch jobs (latency-insensitive)
- Co-location workload patterns
- Resource requests vs. actual usage

**Access:**
- Free for research purposes
- Download after short survey
- Separate mirrors for China/overseas users

**Strengths:**
- Real production workloads from Alibaba datacenters
- Co-location scenario (online + batch) mirrors enterprise reality
- Well-documented schema and processing scripts
- Moderate size (manageable for local processing)

**Challenges:**
- Short duration (8 days for 2018 version)
- Survey required for download
- Different workload model (batch + online vs. steady-state servers)

**Compatibility Score:** 8/10 (Excellent match, manageable size)

---

#### C. MIT Supercloud Dataset

**Source:** https://arxiv.org/pdf/2108.02037

**Coverage:**
- CPU/GPU usage by jobs
- Memory usage patterns
- File system logs (Lustre RPC)
- Physical monitoring data

**Metrics:**
- User process counts
- System load averages
- CPU/memory per-user usage
- File system latency
- Network performance

**Strengths:**
- HPC workload patterns
- Detailed system monitoring
- Academic research quality

**Challenges:**
- HPC-focused (not typical enterprise servers)
- Access process unclear from search results
- Limited documentation found

**Compatibility Score:** 6/10 (Specialized use case)

---

#### D. IEEE DataPort - Cloud System Performance Metrics

**Source:** https://ieee-dataport.org/documents/cloud-stateless-system-performance-metrics-and-status

**Coverage:**
- ~8,000 data points at 5-second intervals
- CPU/memory percentages
- Network traffic (inbound/outbound GB/s)
- Transactions per second (TPS)
- Response time

**Collection:**
- Prometheus-based monitoring
- Custom metric extraction scripts
- Application service integration

**Strengths:**
- High-frequency data (5-second intervals)
- Application performance metrics included
- Prometheus standard (industry-relevant)

**Challenges:**
- Small dataset size (~8K points)
- Short duration
- Single application focus

**Compatibility Score:** 5/10 (Too small, but good for validation)

---

#### E. Kaggle Datasets

**Source:** Multiple Kaggle contributors

**Notable Datasets:**

1. **CPU Performance Metrics (unclean)**
   - https://www.kaggle.com/datasets/mohammedarfathr/cpu-performance-metrics-unclean
   - Simulated CPU performance logs
   - Multiple systems
   - Needs cleaning (good for data quality demo)

2. **Dataset System Resources CPU RAM Disk Network**
   - https://www.kaggle.com/datasets/omnamahshivai/dataset-system-resources-cpu-ram-disk-network
   - Comprehensive resource metrics
   - Unknown size/duration

3. **Cloud Computing Performance Metrics**
   - https://www.kaggle.com/datasets/abdurraziq01/cloud-computing-performance-metrics
   - Cloud-specific monitoring data

**Strengths:**
- Easy access with Kaggle API
- Multiple options
- Often cleaned and preprocessed
- Community documentation

**Challenges:**
- Variable quality
- Often small or toy datasets
- Unclear provenance
- May not represent production workloads

**Compatibility Score:** 6/10 (Good for testing/validation)

---

### 2.3 Dataset Selection Decision Matrix

| Dataset | Size | Duration | Servers | Quality | Access | Use Case Fit | Score |
|---------|------|----------|---------|---------|--------|--------------|-------|
| **Google Cluster 2019** | 2.4TB | 1 month | 1000s | Excellent | BigQuery | Medium | 7/10 |
| **Google Cluster 2011** | 41GB | 29 days | 12.5K | Excellent | Free DL | Medium | 7/10 |
| **Alibaba 2018** | 48GB | 8 days | 4K | Excellent | Free DL | High | 8/10 |
| **MIT Supercloud** | Unknown | Unknown | Unknown | Good | Unclear | Medium | 6/10 |
| **IEEE DataPort** | Small | Short | Few | Good | Free | Low | 5/10 |
| **Kaggle Mixed** | Variable | Variable | Variable | Variable | Easy | Variable | 6/10 |

**Recommendation:** Focus on **Alibaba Cluster Trace v2018** as primary public dataset + Kaggle samples for quick validation.

---

## 3. Selected Datasets

### 3.1 Primary Public Dataset: Alibaba Cluster Trace v2018

**Rationale:**
1. **Best size/quality trade-off:** 48GB compressed is manageable locally
2. **Server-centric:** Unlike Google's task-centric model
3. **Enterprise patterns:** Co-location mirrors real infrastructure optimization
4. **Well-documented:** Active GitHub repo with schema and scripts
5. **Research-proven:** Cited in 100+ papers on capacity planning

**Download Plan:**
1. Complete survey at https://github.com/alibaba/clusterdata
2. Download from overseas mirror
3. Extract relevant subset (2-3 days for initial testing)
4. Transform to project schema

**Transformation Required:**
- Map Alibaba's machine events → server metrics
- Aggregate resource usage → P95 metrics
- Align timestamps to daily granularity
- Extract CPU/memory/disk utilization

---

### 3.2 Secondary Validation: Kaggle Datasets

**Purpose:** Quick experiments and validation

**Selected Datasets:**
1. **Dataset System Resources** - Complete resource monitoring
2. **CPU Performance Metrics** - Data quality demonstration
3. **Cloud Computing Performance** - Cloud-specific patterns

**Usage:**
- Initial prototyping before large dataset download
- Data quality pipeline testing
- Visualization development
- Model training smoke tests

---

### 3.3 Tertiary Option: Google Cluster 2011

**Purpose:** Fallback if Alibaba download fails

**Advantages:**
- Smaller than 2019 version (41GB vs. 2.4TB)
- Direct download available
- Well-documented
- Industry-standard reference

**Transformation Required:**
- More complex (task-level → server-level aggregation)
- Different schema than Alibaba
- May require BigQuery skills for optimal use

---

## 4. Synthetic Data Strategy

### 4.1 Rationale for Synthetic Data

**Primary Reasons:**
1. **Customization:** Tailor seasonality to banking/Citi patterns
2. **Control:** Known ground truth for model validation
3. **Scale:** Generate exactly 120 servers × 4 years as needed
4. **Privacy:** No real customer data concerns
5. **Speed:** Generate on-demand without large downloads
6. **Completeness:** No missing data by design

**Portfolio Benefits:**
- Demonstrates data generation skills
- Shows understanding of time-series patterns
- Proves ability to simulate enterprise scenarios
- Allows reproducible experiments

### 4.2 Current Synthetic Data Design

**Implementation:** [src/utils/data_utils.py](../../src/utils/data_utils.py#L45-L120)

**Function:** `generate_synthetic_server_metrics()`

**Approach:**
```python
metric = base_trend + weekly_seasonality + quarterly_peaks + noise + anomalies
```

**Components:**

1. **Base Trend:** Linear growth simulating capacity growth over time
   - Configurable growth rate (0.5-2% monthly typical)
   - Per-server variation (heterogeneous infrastructure)

2. **Weekly Seasonality:** 7-day cycle
   - Higher utilization weekdays (Mon-Fri)
   - Lower utilization weekends
   - Peak during business hours (9 AM - 5 PM if hourly)

3. **Quarterly Peaks:** Banking-specific
   - End-of-quarter surges (Mar 31, Jun 30, Sep 30, Dec 31)
   - 20-40% increase in load
   - Sustained for 3-5 days

4. **Holiday Effects:** Reduced load
   - US federal holidays
   - Week between Christmas and New Year
   - 30-50% reduction

5. **Noise:** Realistic variability
   - Gaussian noise (σ = 5-10% of base)
   - Per-metric variation

6. **Anomalies (Optional):**
   - Random spikes (1-2% of timestamps)
   - Simulated outages/dips
   - Memory leaks (gradual increase)

**Current Metrics Generated:**
- `cpu_p95`: 10-95%
- `mem_p95`: 15-92%
- `disk_p95`: 5-85%
- `network_in_p95`: 20-500 Mbps
- `network_out_p95`: 10-300 Mbps

### 4.3 Synthetic Data Enhancement Plan

**Phase 1: Improve Realism (Weeks 1-2)**

1. **Correlation Modeling:**
   - CPU ↔ Memory correlation (r = 0.6-0.8 typical)
   - CPU ↔ Network correlation (r = 0.4-0.6)
   - Memory → Disk correlation (memory pressure → swapping)

2. **Server Archetypes:**
   - Web servers (high CPU, moderate memory)
   - Database servers (high memory, high disk)
   - Application servers (balanced)
   - Batch processing (spiky CPU)

3. **Metadata Integration:**
   - Business unit assignment (Trading, Risk, Operations)
   - Criticality levels (Tier 1, 2, 3)
   - Geographic regions (US-East, US-West, EU, APAC)

4. **Advanced Seasonality:**
   - Monthly patterns (month-end reporting)
   - Annual patterns (year-end, tax season)
   - Special events (market volatility → trading surge)

**Phase 2: Anomaly Library (Weeks 3-4)**

1. **Infrastructure Incidents:**
   - Gradual memory leaks
   - CPU throttling events
   - Disk space exhaustion
   - Network saturation

2. **Application Patterns:**
   - Batch job schedules
   - Backup windows
   - Maintenance windows
   - Deployment-related spikes

3. **Labeled Anomalies:**
   - Ground truth labels for anomaly detection training
   - Severity levels (warning, critical)
   - Root cause categories

**Phase 3: Validation Against Real Data (Week 5)**

1. **Statistical Comparison:**
   - Compare distributions (KS test)
   - Autocorrelation function (ACF) matching
   - Spectral density comparison
   - Hurst exponent (long-term memory)

2. **Tune Parameters:**
   - Adjust noise levels to match real data variance
   - Calibrate seasonality amplitudes
   - Refine correlation matrices

---

## 5. Data Integration Plan

### 5.1 Hybrid Data Architecture

**Strategy:** Synthetic-primary with real-data validation

```
Data Sources
├── Synthetic Data (Primary - 80%)
│   ├── Training Set: 120 servers × 3.2 years (Jan 2022 - Mar 2025)
│   ├── Validation Set: 120 servers × 0.6 years (Apr 2025 - Sep 2025)
│   ├── Test Set: 120 servers × 0.2 years (Oct 2025 - Dec 2025)
│   └── Purpose: Model training, hyperparameter tuning, risk analysis
│
└── Real Data (Validation - 20%)
    ├── Alibaba Cluster Trace v2018 (transformed)
    ├── Kaggle Datasets (spot checks)
    └── Purpose: Model validation, realism checks, portfolio credibility
```

### 5.2 Data Pipeline Architecture

**Stage 1: Ingestion**
```
[Synthetic Generator] → CSV → S3 Bronze (raw/)
[Alibaba Download] → Transform → CSV → S3 Bronze (raw/external/)
[Kaggle Download] → Transform → CSV → S3 Bronze (raw/external/)
```

**Stage 2: Standardization**
```python
# Common schema after transformation
standard_schema = {
    'timestamp': datetime,
    'server_id': str,
    'cpu_p95': float,
    'mem_p95': float,
    'disk_p95': float,
    'net_in_p95': float,
    'net_out_p95': float,
    'source': str,  # 'synthetic' | 'alibaba' | 'kaggle'
    'quality_flag': int  # 0 (good), 1 (imputed), 2 (anomaly)
}
```

**Stage 3: Quality Assurance**
- Validate ranges (0-100% for utilization)
- Check for gaps (impute if <5% missing)
- Flag outliers (z-score > 3.0)
- Verify timestamps (no duplicates)

**Stage 4: Feature Engineering**
- Unified feature engineering pipeline
- Same features for synthetic and real data
- Separate tracking for source comparison

**Stage 5: Model Training**
```
Training: 100% synthetic data
Validation: 80% synthetic + 20% real data
Testing: 50% synthetic + 50% real data (model generalization test)
```

### 5.3 Transformation Scripts

**Location:** `scripts/transform_external_data/`

**Scripts to Create:**

1. **`transform_alibaba_trace.py`**
   - Load machine events and task usage
   - Aggregate to server-level metrics
   - Calculate P95 over time windows
   - Map to standard schema

2. **`transform_kaggle_datasets.py`**
   - Load various Kaggle CSVs
   - Standardize column names
   - Resample to daily if needed
   - Map to standard schema

3. **`validate_transformations.py`**
   - Compare distributions pre/post transform
   - Verify no data loss
   - Check schema compliance
   - Generate transformation report

---

## 6. Data Quality Framework

### 6.1 Quality Dimensions

| Dimension | Synthetic Data | Real Data (Alibaba) | Real Data (Kaggle) |
|-----------|----------------|---------------------|-------------------|
| **Completeness** | 100% | >95% (impute gaps) | Variable (80-100%) |
| **Accuracy** | Known ground truth | Production accuracy | Unknown |
| **Consistency** | Perfect (by design) | High | Variable |
| **Timeliness** | Generated on-demand | Historical (2018) | Historical (varies) |
| **Validity** | Schema-compliant | Requires mapping | Requires mapping |
| **Uniqueness** | No duplicates | Check required | Check required |

### 6.2 Quality Checks

**Implementation:** [src/utils/data_utils.py](../../src/utils/data_utils.py#L180-L250)

**Function:** `validate_capacity_df()`

**Checks Performed:**

1. **Schema Validation**
   - Required columns present
   - Correct data types
   - Index is DatetimeIndex

2. **Range Validation**
   - 0 ≤ utilization ≤ 100
   - Positive values for absolute metrics
   - Reasonable network bandwidth ranges

3. **Completeness Check**
   - Missing value percentage < threshold (5%)
   - No gaps > 7 days in time series
   - All servers have minimum data points

4. **Outlier Detection**
   - Z-score method (threshold = 3.0)
   - IQR method for robust outlier detection
   - Flag but don't remove (may be legitimate spikes)

5. **Temporal Consistency**
   - Monotonic timestamps
   - Expected frequency (daily/hourly)
   - No duplicate timestamps per server

### 6.3 Data Quality Reporting

**Output:** `reports/data_quality/quality_report_{timestamp}.html`

**Contents:**
- Summary statistics by source
- Missing data heatmap
- Outlier visualization
- Distribution comparisons
- Correlation matrices
- Temporal coverage charts

---

## 7. Implementation Roadmap

### Phase 1: Synthetic Data Foundation (Week 1) ✅ COMPLETED

**Tasks:**
- [x] Define synthetic data generation function
- [x] Implement basic seasonality (weekly, quarterly)
- [x] Generate sample dataset (30 days, 10 servers)
- [x] Create initial visualizations
- [x] Validate against project requirements

**Deliverables:**
- `src/utils/data_utils.py::generate_synthetic_server_metrics()`
- `data/raw_external/sample_server_metrics.csv`
- `data/raw_external/sample_data_analysis.png`
- `scripts/download_external_data.py` (investigation tool)

**Status:** ✓ Complete

---

### Phase 2: Public Data Download & Exploration (Week 2)

**Tasks:**
- [ ] Download Alibaba Cluster Trace v2018 (2-3 day subset)
- [ ] Download 2-3 Kaggle datasets via API
- [ ] Perform exploratory data analysis (EDA)
- [ ] Document schema differences
- [ ] Assess transformation complexity

**Deliverables:**
- `data/raw_external/alibaba/` with downloaded traces
- `data/raw_external/kaggle/` with selected datasets
- `notebooks/EDA_public_datasets.ipynb` with analysis
- `my_Docs/schema_mapping.md` documentation

**Tools:**
- Kaggle API for automated download
- wget/curl for Alibaba download
- Pandas/seaborn for EDA

---

### Phase 3: Data Transformation Pipeline (Week 3)

**Tasks:**
- [ ] Create transformation script for Alibaba data
- [ ] Create transformation script for Kaggle data
- [ ] Implement standard schema mapping
- [ ] Build quality validation pipeline
- [ ] Generate transformation reports

**Deliverables:**
- `scripts/transform_external_data/transform_alibaba_trace.py`
- `scripts/transform_external_data/transform_kaggle_datasets.py`
- `scripts/transform_external_data/validate_transformations.py`
- Transformed data in `data/processed/external/`

**Quality Gates:**
- All transformed data passes validation
- <5% data loss during transformation
- Schema 100% compliant
- Automated tests pass

---

### Phase 4: Synthetic Data Enhancement (Week 4)

**Tasks:**
- [ ] Implement correlation modeling between metrics
- [ ] Create server archetype system (web, DB, app, batch)
- [ ] Add metadata generation (business unit, criticality, region)
- [ ] Enhance seasonality (monthly, annual patterns)
- [ ] Build anomaly library with labels

**Deliverables:**
- Enhanced `generate_synthetic_server_metrics()` function
- `src/utils/server_archetypes.py` module
- `src/utils/anomaly_generator.py` module
- Updated synthetic data generation (120 servers × 4 years)

**Validation:**
- Compare synthetic vs. real data distributions
- Verify correlations match industry standards
- Test anomaly detection on labeled synthetic anomalies

---

### Phase 5: Hybrid Data Integration (Week 5)

**Tasks:**
- [ ] Upload synthetic data to S3 Bronze layer
- [ ] Upload transformed real data to S3 Bronze layer
- [ ] Implement unified data loader (synthetic + real)
- [ ] Create data source tracking/tagging
- [ ] Build hybrid validation dataset

**Deliverables:**
- S3 bucket populated with raw data
- `src/utils/data_loader.py` with hybrid loading
- Validation dataset: 80% synthetic + 20% real
- Data lineage documentation

**Quality Gates:**
- All data accessible via unified API
- Source tracking works correctly
- No data mixing issues in training sets

---

### Phase 6: Data Quality Dashboard (Week 6)

**Tasks:**
- [ ] Create automated quality reporting
- [ ] Build data profiling dashboard
- [ ] Implement drift detection (synthetic vs. real)
- [ ] Generate weekly quality reports
- [ ] Set up data quality alerts

**Deliverables:**
- `src/visualization.py::generate_quality_dashboard()`
- Interactive Plotly dashboard
- Automated HTML reports
- Quality metrics tracking over time

---

## 8. Success Metrics

### 8.1 Data Quality Metrics

**Target Goals:**

| Metric | Synthetic Data | Real Data | Hybrid |
|--------|----------------|-----------|--------|
| Completeness | 100% | >95% | >97% |
| Schema Compliance | 100% | 100% | 100% |
| Outliers Flagged | <2% | <5% | <3% |
| Missing Values | 0% | <5% | <2% |
| Duplicate Records | 0% | 0% | 0% |

### 8.2 Realism Metrics

**Synthetic Data Validation:**

1. **Distribution Similarity:**
   - KS test p-value > 0.05 vs. real data
   - Mean absolute difference < 10%

2. **Temporal Patterns:**
   - ACF matches real data patterns
   - Seasonality detected (FFT/periodogram)

3. **Correlation Structure:**
   - Correlation matrix within ±0.15 of real data

### 8.3 Project Completion Metrics

**Data Pipeline:**
- [ ] Can generate 120 servers × 4 years in <5 minutes
- [ ] Can load and transform Alibaba data in <30 minutes
- [ ] Can create hybrid validation set in <10 minutes
- [ ] All data quality checks run automatically

**Documentation:**
- [ ] Schema documented for all sources
- [ ] Transformation logic explained
- [ ] Quality report generated weekly
- [ ] Portfolio-ready visualizations created

**ML Readiness:**
- [ ] Training data: 3+ years per server
- [ ] Validation data: 6 months per server
- [ ] Test data: 3 months per server
- [ ] Feature engineering pipeline tested on all sources

---

## 9. Data Sources Summary

### Primary Sources (80% of usage)

1. **Synthetic Data Generation**
   - Location: `src/utils/data_utils.py::generate_synthetic_server_metrics()`
   - Volume: 120 servers × 4 years × daily = 175,200 records
   - Cost: $0 (generated locally)
   - Status: Foundation complete, enhancements planned

### Secondary Sources (20% of usage)

2. **Alibaba Cluster Trace v2018**
   - URL: https://github.com/alibaba/clusterdata
   - Volume: 48GB compressed, subset to be used
   - Cost: Free (after survey)
   - Status: To be downloaded in Phase 2

3. **Kaggle Datasets**
   - API: kaggle datasets download
   - Volume: Variable (typically <1GB each)
   - Cost: Free
   - Status: To be downloaded in Phase 2

### Tertiary Sources (Optional)

4. **Google Cluster Data 2011**
   - URL: https://github.com/google/cluster-data
   - Volume: 41GB compressed
   - Cost: Free
   - Status: Backup option

5. **IEEE DataPort Datasets**
   - URL: https://ieee-dataport.org
   - Volume: Small (<100MB typically)
   - Cost: Free
   - Status: For spot validation

---

## 10. Next Actions

### Immediate (This Week)

1. **Download Alibaba Trace:**
   - Complete survey
   - Download 2-3 day subset (test)
   - Store in `data/raw_external/alibaba/`

2. **Setup Kaggle API:**
   - Install kaggle CLI
   - Configure API key
   - Download 2-3 datasets

3. **Create EDA Notebook:**
   - `notebooks/02_EDA_external_data.ipynb`
   - Compare Alibaba vs. synthetic
   - Document findings

### Short-term (Next 2 Weeks)

4. **Build Transformation Pipeline:**
   - Alibaba → standard schema
   - Kaggle → standard schema
   - Quality validation

5. **Enhance Synthetic Data:**
   - Add correlation modeling
   - Implement server archetypes
   - Generate full 4-year dataset

6. **Upload to S3:**
   - Create S3 bucket if not exists
   - Upload raw synthetic data
   - Upload transformed real data

### Medium-term (Weeks 4-6)

7. **Integrate with ML Pipeline:**
   - Update `src/data_generation.py` to use hybrid data
   - Modify `src/etl_pipeline.py` for dual sources
   - Test model training on both sources

8. **Create Quality Dashboard:**
   - Automated quality reports
   - Source comparison visualizations
   - Weekly quality metrics

---

## Appendix A: Data Schema

### Standard Schema (Post-Transformation)

```python
schema = {
    'timestamp': 'datetime64[ns]',        # UTC timestamp
    'server_id': 'str',                   # Unique server identifier
    'cpu_p95': 'float64',                 # CPU P95 utilization (0-100%)
    'mem_p95': 'float64',                 # Memory P95 utilization (0-100%)
    'disk_p95': 'float64',                # Disk P95 utilization (0-100%)
    'net_in_p95': 'float64',              # Network In P95 (Mbps)
    'net_out_p95': 'float64',             # Network Out P95 (Mbps)

    # Metadata
    'source': 'str',                      # 'synthetic' | 'alibaba' | 'kaggle' | 'google'
    'quality_flag': 'int8',               # 0 (good) | 1 (imputed) | 2 (flagged)
    'business_unit': 'str',               # Trading | Risk | Operations | IT
    'criticality': 'str',                 # Tier1 | Tier2 | Tier3
    'region': 'str',                      # US-East | US-West | EU | APAC
    'server_type': 'str',                 # web | db | app | batch
}
```

### Alibaba Raw Schema (Example)

```python
# machine_events.csv (subset)
{
    'timestamp': int64,                   # Unix timestamp
    'machine_id': int64,                  # Machine ID
    'event_type': str,                    # add | remove | update
    'cpu_num': int32,                     # Number of CPUs
    'mem_size': int64,                    # Memory in bytes
}

# task_usage.csv (subset)
{
    'start_time': int64,                  # Window start (Unix)
    'end_time': int64,                    # Window end (Unix)
    'machine_id': int64,                  # Machine ID
    'cpu_util': float64,                  # CPU utilization (0-100)
    'mem_util': float64,                  # Memory utilization (0-100)
}
```

**Transformation Required:**
1. Convert Unix timestamp → datetime
2. Map machine_id → server_id
3. Aggregate task_usage → P95 metrics per machine per day
4. Join with machine_events for metadata

---

## Appendix B: References

**Public Dataset Sources:**
- [Google Cluster Data](https://github.com/google/cluster-data)
- [Alibaba Cluster Data](https://github.com/alibaba/clusterdata)
- [MIT Supercloud Dataset](https://arxiv.org/pdf/2108.02037)
- [IEEE DataPort - Cloud Metrics](https://ieee-dataport.org/documents/cloud-stateless-system-performance-metrics-and-status)
- [Kaggle Infrastructure Datasets](https://www.kaggle.com/datasets)

**Academic References:**
- "Public Datasets for Cloud Computing: A Comprehensive Survey" - ACM Computing Surveys 2026
- "A Deep Dive into the Google Cluster Workload Traces" - arXiv:2308.02358
- "Analyzing Alibaba's Co-located Datacenter Workloads" - BigData 2018

**Tools:**
- Kaggle API: https://github.com/Kaggle/kaggle-api
- Google BigQuery: https://cloud.google.com/bigquery
- Alibaba Processing Scripts: https://github.com/alibaba/clusterdata/tree/master/scripts

---

**Document Status:** Draft for Review
**Next Review Date:** 2026-01-29
**Approvers:** Project Lead, Data Engineering Lead

---

**Changelog:**
- 2026-01-22: Initial draft (v1.0) - Data landscape analysis and strategy formulation
