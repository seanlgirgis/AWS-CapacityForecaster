# AWS-CapacityForecaster: Master Technical Guide
## Synthetic Data Generation System - Complete Documentation

**Document Version:** 1.0
**Last Updated:** 2026-01-22
**Author:** Sean L. Girgis
**System Status:** Production-Ready

---

## Table of Contents

1. [Executive Overview](#1-executive-overview)
2. [System Architecture](#2-system-architecture)
3. [Data Generation Process Flow](#3-data-generation-process-flow)
4. [Component Deep Dive](#4-component-deep-dive)
5. [Configuration System](#5-configuration-system)
6. [Code Files Reference](#6-code-files-reference)
7. [Function Catalog](#7-function-catalog)
8. [Input/Output Specifications](#8-inputoutput-specifications)
9. [Success Criteria & Validation](#9-success-criteria--validation)
10. [Troubleshooting Guide](#10-troubleshooting-guide)

---

## 1. Executive Overview

### 1.1 Purpose

This document provides complete technical documentation for the **Synthetic Data Generation System** within the AWS-CapacityForecaster project. The system generates enterprise-realistic server capacity metrics for machine learning model training and validation.

### 1.2 What This System Does

**Input:**
- Configuration parameters (YAML)
- Date ranges, server counts, metrics specifications

**Process:**
- Assigns server archetypes (Web, Database, Application, Batch)
- Generates correlated time-series metrics (CPU, Memory, Disk, Network)
- Applies realistic patterns (business hours, quarterly peaks, holidays)
- Adds business metadata and calendar features

**Output:**
- CSV/Parquet files with 175,320+ records
- Visualizations and quality reports
- Production-ready datasets for ML training

### 1.3 Key Features

âœ… **4 Server Archetypes** - Web, Database, Application, Batch with unique behaviors
âœ… **Correlated Metrics** - Cholesky decomposition for realistic metric relationships
âœ… **Banking Seasonality** - Quarterly peaks, holiday effects, weekly patterns
âœ… **Metadata Integration** - Business unit, criticality, region, server type
âœ… **Scalable** - 50-200 servers, 1-5 years of data
âœ… **Reproducible** - Seeded random generation for consistent results

### 1.4 Generated Dataset Summary

| Attribute | Value |
|-----------|-------|
| **Records** | 175,320 (120 servers Ã— 1,461 days) |
| **Date Range** | 2022-01-01 to 2025-12-31 (4 years) |
| **Granularity** | Daily (hourly optional) |
| **Metrics** | 5 (CPU, Memory, Disk, Network In/Out) |
| **Metadata Columns** | 13 (server info, business context, calendar) |
| **File Size** | 3.04 MB (compressed) |
| **Quality** | 100% complete, 0 missing values in metrics |

---

## 2. System Architecture

### 2.1 High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Configuration Layer                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚ config.yaml  â”‚  â”‚ Environment  â”‚  â”‚ CLI Args     â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Archetype Assignment Layer                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  assign_archetypes_to_fleet()                            â”‚  â”‚
â”‚  â”‚  - Distributes archetypes across 120 servers             â”‚  â”‚
â”‚  â”‚  - Web: 35%, App: 40%, DB: 15%, Batch: 10%             â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Time Series Generation Layer                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚  Web       â”‚  â”‚  Database  â”‚  â”‚  Applicationâ”‚               â”‚
â”‚  â”‚  Archetype â”‚  â”‚  Archetype â”‚  â”‚  Archetype  â”‚   Batch       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   Archetype    â”‚
â”‚         â†“                â†“                â†“            â†“         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  generate_correlated_metrics()                           â”‚  â”‚
â”‚  â”‚  - Cholesky decomposition for correlation               â”‚  â”‚
â”‚  â”‚  - Time-based factors (business hours, weekends)        â”‚  â”‚
â”‚  â”‚  - Seasonal patterns (quarterly, holidays)              â”‚  â”‚
â”‚  â”‚  - Spike modeling                                        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Metadata Enrichment Layer                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  generate_server_metadata()                              â”‚  â”‚
â”‚  â”‚  - Business unit assignment                              â”‚  â”‚
â”‚  â”‚  - Criticality levels                                    â”‚  â”‚
â”‚  â”‚  - Geographic regions                                    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  add_calendar_features()                                 â”‚  â”‚
â”‚  â”‚  - Year, month, quarter, day of week                    â”‚  â”‚
â”‚  â”‚  - Weekend flags, end-of-quarter flags                  â”‚  â”‚
â”‚  â”‚  - US holiday indicators                                â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Output & Validation Layer                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚  CSV.gz File â”‚  â”‚  Visualizationsâ”‚  â”‚  Quality     â”‚         â”‚
â”‚  â”‚  175K recordsâ”‚  â”‚  7 panels      â”‚  â”‚  Reports     â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 Data Flow Diagram

```
START
  â”‚
  â”œâ”€â†’ Load Config (config.yaml)
  â”‚     â””â”€â†’ Validate parameters
  â”‚
  â”œâ”€â†’ Generate Date Range
  â”‚     â””â”€â†’ 2022-01-01 to 2025-12-31 (daily)
  â”‚
  â”œâ”€â†’ Assign Server Archetypes
  â”‚     â”œâ”€â†’ Web servers (42)
  â”‚     â”œâ”€â†’ Application servers (48)
  â”‚     â”œâ”€â†’ Database servers (18)
  â”‚     â””â”€â†’ Batch servers (12)
  â”‚
  â”œâ”€â†’ FOR EACH Server:
  â”‚     â”‚
  â”‚     â”œâ”€â†’ Create Archetype Instance
  â”‚     â”‚     â””â”€â†’ Load profile (base metrics, correlations, patterns)
  â”‚     â”‚
  â”‚     â””â”€â†’ FOR EACH Timestamp:
  â”‚           â”‚
  â”‚           â”œâ”€â†’ Calculate Time Factors
  â”‚           â”‚     â”œâ”€â†’ Business hours multiplier
  â”‚           â”‚     â”œâ”€â†’ Weekend adjustment
  â”‚           â”‚     â”œâ”€â†’ Quarterly peak factor
  â”‚           â”‚     â””â”€â†’ Holiday effect
  â”‚           â”‚
  â”‚           â”œâ”€â†’ Generate Correlated Metrics
  â”‚           â”‚     â”œâ”€â†’ Build correlation matrix (4Ã—4)
  â”‚           â”‚     â”œâ”€â†’ Cholesky decomposition
  â”‚           â”‚     â”œâ”€â†’ Generate random vector
  â”‚           â”‚     â”œâ”€â†’ Transform to correlated values
  â”‚           â”‚     â””â”€â†’ Scale by variance + add to base
  â”‚           â”‚
  â”‚           â”œâ”€â†’ Apply Spike Logic
  â”‚           â”‚     â””â”€â†’ Random spike if probability < threshold
  â”‚           â”‚
  â”‚           â”œâ”€â†’ Clip to Valid Ranges
  â”‚           â”‚     â”œâ”€â†’ CPU/Mem/Disk: 0-100%
  â”‚           â”‚     â””â”€â†’ Network: 0-1000 Mbps
  â”‚           â”‚
  â”‚           â””â”€â†’ Create Record
  â”‚                 â””â”€â†’ {timestamp, server_id, cpu, mem, disk, net_in, net_out}
  â”‚
  â”œâ”€â†’ Build DataFrame (175,320 rows Ã— 6 columns)
  â”‚
  â”œâ”€â†’ Add Business Metadata
  â”‚     â”œâ”€â†’ Generate 120 server metadata records
  â”‚     â””â”€â†’ Merge on server_id
  â”‚
  â”œâ”€â†’ Add Calendar Features
  â”‚     â”œâ”€â†’ Extract year, month, quarter from timestamp
  â”‚     â”œâ”€â†’ Calculate day of week, weekend flags
  â”‚     â”œâ”€â†’ Identify end-of-quarter dates
  â”‚     â””â”€â†’ Mark US holidays
  â”‚
  â”œâ”€â†’ Validate Data Quality
  â”‚     â”œâ”€â†’ Check for missing values
  â”‚     â”œâ”€â†’ Verify ranges (0-100%)
  â”‚     â””â”€â†’ Log statistics
  â”‚
  â””â”€â†’ Save Output
        â”œâ”€â†’ Compress to CSV.gz (3.04 MB)
        â””â”€â†’ Generate visualizations
              â””â”€â†’ 7-panel dashboard

END
```

### 2.3 Module Dependency Graph

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  config.yaml   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  src/utils/    â”‚
                    â”‚  config.py     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚                 â”‚                 â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ src/utils/       â”‚ â”‚ src/utils/â”‚ â”‚ src/utils/     â”‚
  â”‚ server_          â”‚ â”‚ data_     â”‚ â”‚ ml_utils.py    â”‚
  â”‚ archetypes.py    â”‚ â”‚ utils.py  â”‚ â”‚ (validation)   â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                 â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚  src/             â”‚
           â”‚  data_generation  â”‚
           â”‚  .py              â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚                 â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ data/        â”‚  â”‚ scripts/       â”‚
  â”‚ synthetic/   â”‚  â”‚ visualize_     â”‚
  â”‚ *.csv.gz     â”‚  â”‚ synthetic_data â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3. Data Generation Process Flow

### 3.1 End-to-End Process Timeline

```
Time: 0s
â”œâ”€ START: python -m src.data_generation
â”œâ”€ [0.1s] Load configuration from config.yaml
â”œâ”€ [0.2s] Validate parameters (120 servers, 4 years)
â”œâ”€ [0.3s] Generate date range (1,461 timestamps)
â”œâ”€ [0.5s] Assign archetypes to 120 servers
â”‚
Time: 1s
â”œâ”€ [1-15s] Generate metrics for all servers
â”‚   â”œâ”€ Progress: server_000... (0%)
â”‚   â”œâ”€ Progress: server_020... (16%)
â”‚   â”œâ”€ Progress: server_040... (33%)
â”‚   â”œâ”€ Progress: server_060... (50%)
â”‚   â”œâ”€ Progress: server_080... (66%)
â”‚   â”œâ”€ Progress: server_100... (83%)
â”‚   â””â”€ Complete: 175,320 records generated
â”‚
Time: 15s
â”œâ”€ [15-16s] Build DataFrame (17.89 MB in memory)
â”œâ”€ [16-17s] Generate metadata (120 records)
â”œâ”€ [17-18s] Merge metadata with metrics
â”œâ”€ [18-19s] Add calendar features (7 columns)
â”œâ”€ [19-20s] Validate data quality
â”‚   â”œâ”€ Missing values: 0
â”‚   â”œâ”€ Valid ranges: 100%
â”‚   â””â”€ Statistics: MAE, std, min, max
â”‚
Time: 20s
â”œâ”€ [20-22s] Compress and save CSV.gz (3.04 MB)
â””â”€ [22s] COMPLETE
```

### 3.2 Detailed Function Call Stack

```
main()
  â”‚
  â”œâ”€â†’ parse_args()  # CLI argument parsing
  â”‚     â””â”€â†’ Returns: {output, servers, years, ...}
  â”‚
  â”œâ”€â†’ generate_full_dataset()
  â”‚     â”‚
  â”‚     â”œâ”€â†’ get_data_config()  # Load from config.yaml
  â”‚     â”‚     â””â”€â†’ Returns: {num_servers: 120, start_date: '2022-01-01', ...}
  â”‚     â”‚
  â”‚     â”œâ”€â†’ pd.date_range(start='2022-01-01', end='2025-12-31', freq='D')
  â”‚     â”‚     â””â”€â†’ Returns: DatetimeIndex with 1,461 dates
  â”‚     â”‚
  â”‚     â”œâ”€â†’ assign_archetypes_to_fleet(num_servers=120)
  â”‚     â”‚     â”‚
  â”‚     â”‚     â”œâ”€â†’ Calculate counts: web=42, app=48, db=18, batch=12
  â”‚     â”‚     â””â”€â†’ Returns: {'server_000': 'web', 'server_001': 'web', ...}
  â”‚     â”‚
  â”‚     â”œâ”€â†’ FOR each server_id in archetype_assignments:
  â”‚     â”‚     â”‚
  â”‚     â”‚     â”œâ”€â†’ get_archetype(server_type='web', server_id='server_000')
  â”‚     â”‚     â”‚     â”‚
  â”‚     â”‚     â”‚     â”œâ”€â†’ Create ServerArchetype instance
  â”‚     â”‚     â”‚     â”œâ”€â†’ Load profile from ARCHETYPE_PROFILES
  â”‚     â”‚     â”‚     â””â”€â†’ Returns: ServerArchetype object
  â”‚     â”‚     â”‚
  â”‚     â”‚     â””â”€â†’ FOR each timestamp in date_range:
  â”‚     â”‚           â”‚
  â”‚     â”‚           â”œâ”€â†’ archetype.get_time_factor(timestamp)
  â”‚     â”‚           â”‚     â”‚
  â”‚     â”‚           â”‚     â”œâ”€â†’ hour = timestamp.hour (0-23)
  â”‚     â”‚           â”‚     â”œâ”€â†’ dayofweek = timestamp.dayofweek (0-6)
  â”‚     â”‚           â”‚     â”œâ”€â†’ IF 9 <= hour <= 17: bh_factor = profile.business_hours_factor
  â”‚     â”‚           â”‚     â”œâ”€â†’ IF dayofweek >= 5: weekend_factor = profile.weekend_factor
  â”‚     â”‚           â”‚     â””â”€â†’ Returns: bh_factor * weekend_factor
  â”‚     â”‚           â”‚
  â”‚     â”‚           â”œâ”€â†’ _get_quarterly_peak_factor(timestamp, config)
  â”‚     â”‚           â”‚     â”‚
  â”‚     â”‚           â”‚     â”œâ”€â†’ IF month in [3,6,9,12] AND day > (days_in_month - 5):
  â”‚     â”‚           â”‚     â”‚     â””â”€â†’ Returns: 1.0 + (0.3 * proximity_to_quarter_end)
  â”‚     â”‚           â”‚     â””â”€â†’ ELSE: Returns: 1.0
  â”‚     â”‚           â”‚
  â”‚     â”‚           â”œâ”€â†’ _get_holiday_factor(timestamp, config)
  â”‚     â”‚           â”‚     â”‚
  â”‚     â”‚           â”‚     â”œâ”€â†’ IF month==1 AND day==1: Returns: 0.5 (New Year)
  â”‚     â”‚           â”‚     â”œâ”€â†’ IF month==12 AND day>=25: Returns: 0.6 (Xmas week)
  â”‚     â”‚           â”‚     â””â”€â†’ ELSE: Returns: 1.0
  â”‚     â”‚           â”‚
  â”‚     â”‚           â”œâ”€â†’ combined_factor = time_factor * qtr_factor * holiday_factor
  â”‚     â”‚           â”‚
  â”‚     â”‚           â”œâ”€â†’ trend_factor = timestamp_index / total_timestamps
  â”‚     â”‚           â”‚
  â”‚     â”‚           â”œâ”€â†’ archetype.generate_correlated_metrics(timestamp, combined_factor, trend_factor)
  â”‚     â”‚           â”‚     â”‚
  â”‚     â”‚           â”‚     â”œâ”€â†’ Build correlation matrix (4Ã—4):
  â”‚     â”‚           â”‚     â”‚     [1.0,  cpu_mem_corr,  0.1,  cpu_net_corr]
  â”‚     â”‚           â”‚     â”‚     [cpu_mem_corr,  1.0,  mem_disk_corr,  0.2]
  â”‚     â”‚           â”‚     â”‚     [0.1,  mem_disk_corr,  1.0,  0.3]
  â”‚     â”‚           â”‚     â”‚     [cpu_net_corr,  0.2,  0.3,  1.0]
  â”‚     â”‚           â”‚     â”‚
  â”‚     â”‚           â”‚     â”œâ”€â†’ np.linalg.cholesky(corr_matrix)
  â”‚     â”‚           â”‚     â”‚     â””â”€â†’ Returns: Lower triangular matrix L
  â”‚     â”‚           â”‚     â”‚
  â”‚     â”‚           â”‚     â”œâ”€â†’ z = self.rng.randn(4)  # Independent standard normal
  â”‚     â”‚           â”‚     â”‚
  â”‚     â”‚           â”‚     â”œâ”€â†’ correlated = L @ z  # Matrix multiplication
  â”‚     â”‚           â”‚     â”‚
  â”‚     â”‚           â”‚     â”œâ”€â†’ cpu = base_cpu * time_factor * (1 + trend) + correlated[0] * variance
  â”‚     â”‚           â”‚     â”œâ”€â†’ memory = base_mem * time_factor * (1 + trend) + correlated[1] * variance
  â”‚     â”‚           â”‚     â”œâ”€â†’ disk = base_disk * (1 + trend) + correlated[2] * variance
  â”‚     â”‚           â”‚     â”œâ”€â†’ network = base_net * time_factor + correlated[3] * variance
  â”‚     â”‚           â”‚     â”‚
  â”‚     â”‚           â”‚     â”œâ”€â†’ IF random() < spike_probability:
  â”‚     â”‚           â”‚     â”‚     â”œâ”€â†’ cpu *= spike_magnitude
  â”‚     â”‚           â”‚     â”‚     â”œâ”€â†’ memory *= (spike_magnitude * 0.7)
  â”‚     â”‚           â”‚     â”‚     â””â”€â†’ network *= (spike_magnitude * 0.8)
  â”‚     â”‚           â”‚     â”‚
  â”‚     â”‚           â”‚     â”œâ”€â†’ Clip all values to valid ranges (0-100%, 0-1000 Mbps)
  â”‚     â”‚           â”‚     â”‚
  â”‚     â”‚           â”‚     â””â”€â†’ Returns: {cpu_p95, mem_p95, disk_p95, net_in_p95, net_out_p95}
  â”‚     â”‚           â”‚
  â”‚     â”‚           â””â”€â†’ Append record to all_data list
  â”‚     â”‚
  â”‚     â”œâ”€â†’ df = pd.DataFrame(all_data)
  â”‚     â”‚     â””â”€â†’ Returns: DataFrame with 175,320 rows Ã— 6 columns
  â”‚     â”‚
  â”‚     â”œâ”€â†’ generate_server_metadata(n_servers=120)
  â”‚     â”‚     â”‚
  â”‚     â”‚     â”œâ”€â†’ FOR each server_id:
  â”‚     â”‚     â”‚     â”œâ”€â†’ Assign random business_unit from ['Trading', 'Retail', 'Compliance', 'IT']
  â”‚     â”‚     â”‚     â”œâ”€â†’ Assign random criticality from ['High', 'Medium', 'Low']
  â”‚     â”‚     â”‚     â”œâ”€â†’ Assign random region from ['US-East', 'US-West', 'EU', 'Asia']
  â”‚     â”‚     â”‚     â””â”€â†’ Generate app_name: f"{bu}-app-{i}"
  â”‚     â”‚     â”‚
  â”‚     â”‚     â””â”€â†’ Returns: DataFrame with 120 rows Ã— 5 columns
  â”‚     â”‚
  â”‚     â”œâ”€â†’ metadata_df['server_type'] = metadata_df['server_id'].map(archetype_assignments)
  â”‚     â”‚
  â”‚     â”œâ”€â†’ df = df.merge(metadata_df, on='server_id')
  â”‚     â”‚     â””â”€â†’ Returns: DataFrame with 175,320 rows Ã— 11 columns
  â”‚     â”‚
  â”‚     â”œâ”€â†’ add_calendar_features(df, date_col='timestamp')
  â”‚     â”‚     â”‚
  â”‚     â”‚     â”œâ”€â†’ df['year'] = df['timestamp'].dt.year
  â”‚     â”‚     â”œâ”€â†’ df['month'] = df['timestamp'].dt.month
  â”‚     â”‚     â”œâ”€â†’ df['quarter'] = df['timestamp'].dt.quarter
  â”‚     â”‚     â”œâ”€â†’ df['dayofweek'] = df['timestamp'].dt.dayofweek
  â”‚     â”‚     â”œâ”€â†’ df['is_weekend'] = (df['dayofweek'] >= 5).astype(int)
  â”‚     â”‚     â”œâ”€â†’ df['is_eoq'] = df['timestamp'].dt.is_quarter_end.astype(int)
  â”‚     â”‚     â”œâ”€â†’ df['is_holiday'] = df['timestamp'].apply(check_us_holiday)
  â”‚     â”‚     â”‚
  â”‚     â”‚     â””â”€â†’ Returns: DataFrame with 175,320 rows Ã— 18 columns
  â”‚     â”‚
  â”‚     â””â”€â†’ Returns: Final DataFrame
  â”‚
  â”œâ”€â†’ save_dataset(df, output_path='data/synthetic/server_metrics_full.csv', compress=True)
  â”‚     â”‚
  â”‚     â”œâ”€â†’ df.to_csv(output_path + '.gz', compression='gzip')
  â”‚     â””â”€â†’ Log file size and completion
  â”‚
  â””â”€â†’ COMPLETE

```

---

## 4. Component Deep Dive

### 4.1 Server Archetype System (`src/utils/server_archetypes.py`)

#### 4.1.1 Purpose

The archetype system creates **heterogeneous infrastructure** by defining 4 distinct server types with unique:
- Resource utilization patterns
- Correlation structures
- Time-based behaviors
- Spike characteristics

#### 4.1.2 Archetype Profiles

**Profile Structure:**
```python
@dataclass
class ArchetypeProfile:
    name: str

    # Base metrics (mean utilization)
    base_cpu: float          # e.g., 45% for web servers
    base_memory: float       # e.g., 35% for web servers
    base_disk: float
    base_network: float

    # Variance (standard deviation)
    cpu_variance: float      # e.g., 15% std dev
    memory_variance: float
    disk_variance: float
    network_variance: float

    # Correlations (Pearson correlation coefficients)
    cpu_memory_correlation: float     # e.g., 0.5 for web
    cpu_network_correlation: float    # e.g., 0.8 for web (high!)
    memory_disk_correlation: float    # e.g., 0.7 for database

    # Time-based multipliers
    business_hours_factor: float      # e.g., 1.6x during 9-5
    weekend_factor: float             # e.g., 0.5x on weekends

    # Spike modeling
    spike_probability: float          # e.g., 0.03 (3% of time)
    spike_magnitude: float            # e.g., 1.8x (80% increase)

    # Growth trend
    monthly_growth_rate: float        # e.g., 0.5% per month
```

**Web Server Profile:**
```python
ServerType.WEB: ArchetypeProfile(
    name="Web Server",
    base_cpu=45.0,           # Moderate CPU for request processing
    base_memory=35.0,        # Lower memory (stateless)
    base_disk=20.0,          # Minimal disk I/O
    base_network=150.0,      # High network (HTTP traffic)

    cpu_variance=15.0,
    memory_variance=8.0,
    disk_variance=5.0,
    network_variance=50.0,

    cpu_memory_correlation=0.5,    # Moderate: CPU driven by requests
    cpu_network_correlation=0.8,   # STRONG: requests drive both CPU and network
    memory_disk_correlation=0.2,   # Weak: little caching

    business_hours_factor=1.6,     # HIGH sensitivity to business hours
    weekend_factor=0.5,            # 50% reduction on weekends

    spike_probability=0.03,        # 3% chance per hour
    spike_magnitude=1.8,           # 80% increase during spike

    monthly_growth_rate=0.5,       # 0.5% growth per month
)
```

**Database Server Profile:**
```python
ServerType.DATABASE: ArchetypeProfile(
    name="Database Server",
    base_cpu=35.0,           # Lower CPU (optimized queries)
    base_memory=70.0,        # HIGH memory (caching, buffer pools)
    base_disk=55.0,          # HIGH disk I/O
    base_network=100.0,      # Moderate network

    cpu_variance=12.0,
    memory_variance=10.0,
    disk_variance=15.0,
    network_variance=30.0,

    cpu_memory_correlation=0.6,    # Memory pressure affects CPU
    cpu_network_correlation=0.4,   # Moderate coupling
    memory_disk_correlation=0.7,   # STRONG: memory pressure â†’ swapping â†’ disk I/O

    business_hours_factor=1.3,     # Moderate sensitivity
    weekend_factor=0.7,            # Still active on weekends

    spike_probability=0.01,        # LOW: steady-state operation
    spike_magnitude=1.4,           # Smaller spikes

    monthly_growth_rate=1.0,       # Data grows steadily
)
```

**Application Server Profile:**
```python
ServerType.APPLICATION: ArchetypeProfile(
    name="Application Server",
    base_cpu=50.0,           # Balanced
    base_memory=55.0,        # Balanced
    base_disk=30.0,          # Balanced
    base_network=120.0,      # Balanced

    cpu_variance=18.0,       # Higher variability
    memory_variance=15.0,
    disk_variance=10.0,
    network_variance=40.0,

    cpu_memory_correlation=0.7,    # Strong coupling (stateful apps)
    cpu_network_correlation=0.6,   # Moderate
    memory_disk_correlation=0.4,   # Some caching

    business_hours_factor=1.5,     # Strong business hours pattern
    weekend_factor=0.6,            # 40% reduction on weekends

    spike_probability=0.02,        # Moderate spikes
    spike_magnitude=1.6,

    monthly_growth_rate=0.8,
)
```

**Batch Processing Server Profile:**
```python
ServerType.BATCH: ArchetypeProfile(
    name="Batch Processing Server",
    base_cpu=30.0,           # LOW baseline (idle between jobs)
    base_memory=45.0,        # Moderate
    base_disk=40.0,          # High I/O during processing
    base_network=80.0,       # Lower network

    cpu_variance=25.0,       # VERY HIGH variance (spiky workload)
    memory_variance=12.0,
    disk_variance=20.0,
    network_variance=35.0,

    cpu_memory_correlation=0.4,    # Weaker: batch jobs are diverse
    cpu_network_correlation=0.3,
    memory_disk_correlation=0.5,

    business_hours_factor=0.8,     # INVERSE: lower during business hours
    weekend_factor=1.2,            # HIGHER on weekends (batch windows)

    spike_probability=0.08,        # VERY HIGH: 8% (scheduled jobs)
    spike_magnitude=2.5,           # LARGE: 150% increase

    monthly_growth_rate=0.3,       # Slower growth
)
```

#### 4.1.3 Correlation Matrix Construction

For each archetype, we build a **4Ã—4 correlation matrix** for [CPU, Memory, Disk, Network]:

```python
corr_matrix = np.array([
    [1.0, cpu_memory_corr, 0.1, cpu_network_corr],
    [cpu_memory_corr, 1.0, memory_disk_corr, 0.2],
    [0.1, memory_disk_corr, 1.0, 0.3],
    [cpu_network_corr, 0.2, 0.3, 1.0]
])
```

**Cholesky Decomposition:**
To generate correlated random variables:

1. Decompose correlation matrix: `L = cholesky(corr_matrix)`
2. Generate independent random vector: `z ~ N(0, 1)` (4 values)
3. Transform to correlated: `correlated = L @ z`
4. Scale by variance: `metric = base + correlated[i] * variance`

**Example for Web Server:**
```
corr_matrix = [
    [1.0,  0.5,  0.1,  0.8],  # CPU
    [0.5,  1.0,  0.2,  0.2],  # Memory
    [0.1,  0.2,  1.0,  0.3],  # Disk
    [0.8,  0.2,  0.3,  1.0]   # Network
]

Cholesky(corr_matrix) = L = [
    [1.0,   0.0,   0.0,   0.0],
    [0.5,   0.866, 0.0,   0.0],
    [0.1,   0.173, 0.970, 0.0],
    [0.8,  -0.260, 0.231, 0.503]
]

If z = [0.5, -0.3, 0.8, -0.2], then:
correlated = L @ z = [0.5, 0.11, -0.023, 0.45]

cpu = 45 * time_factor + 0.5 * 15 = 45 * time_factor + 7.5
mem = 35 * time_factor + 0.11 * 8 = 35 * time_factor + 0.88
disk = 20 + (-0.023) * 5 = 20 - 0.12
network = 150 * time_factor + 0.45 * 50 = 150 * time_factor + 22.5
```

This ensures that when CPU is high, Network is also likely high (0.8 correlation).

---

*[Document continues in next file due to length...]*

---

## Navigation

ğŸ“„ **Current:** Master Technical Guide
ğŸ“„ **Next:** [01_Configuration_System.md](01_Configuration_System.md)
ğŸ“„ **Next:** [02_Code_Walkthrough.md](02_Code_Walkthrough.md)
ğŸ“„ **Next:** [03_Function_Catalog.md](03_Function_Catalog.md)

---

**Document End - Part 1 of 5**
