# AWS-CapacityForecaster: Project Next Steps

## Current Status
**Data Generation Complete** — 120 servers with realistic synthetic metrics (CPU, memory, disk_io, network) across 4 years with growth trends, diurnal patterns, quarterly seasonality, and correlated noise.

---

## Pipeline Stages Overview

| Stage | Description | Status | Key Files/Tools |
|-------|-------------|--------|-----------------|
| **1. Data Upload to S3** | Push generated CSVs to S3 bucket with proper partitioning | **READY** | `scripts/push_to_s3.py`, boto3 |
| **2. Data Validation/EDA** | Verify patterns look realistic — plot samples, check correlations, confirm quarterly spikes | Pending | Jupyter notebook, matplotlib/seaborn |
| **3. Feature Engineering** | Create ML-ready features: lags, rolling means, `days_to_quarter_end`, `is_business_hour`, trend decomposition | Pending | pandas, `feature_engineering.py` |
| **4. Model Training** | Train forecasting models — Prophet (seasonal), XGBoost (feature-based), optionally Amazon Forecast | Pending | Prophet, scikit-learn, SageMaker |
| **5. Model Evaluation** | Compare accuracy (MAE, RMSE), especially during peak periods (Q-end) | Pending | Cross-validation, holdout test set |
| **6. Capacity Risk Flagging** | Flag servers predicted to exceed thresholds (e.g., P95 > 85%) in next 30/90 days | Pending | Risk classification logic |
| **7. Reporting/Dashboard** | Visualize forecasts, at-risk servers, recommendations | Pending | Streamlit, QuickSight, or static reports |

---

## Stage 1: Upload to S3 (READY)

### Configuration Verified
```yaml
# config/config.yaml
data:
  generated_data_path: data/synthetic/server_metrics.csv.gz
  compression: true                       # GZIP compression for S3 efficiency

aws:
  bucket_name: sean-capacity-forecast-data
  raw_prefix: raw/server_metrics/
  profile: study
```

### Execute Upload
```bash
python scripts/push_to_s3.py
```

**Expected Result:**
```
s3://sean-capacity-forecast-data/raw/server_metrics/server_metrics.csv.gz
```

---

## Stage 2: Data Validation / EDA

### Goals
- Confirm synthetic data has expected patterns
- Visualize sample servers by archetype
- Check correlation matrices match expected values
- Verify quarterly spikes appear at correct times

### Suggested Notebook: `notebooks/01_data_validation.ipynb`

```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load data
df = pd.read_csv('data/synthetic/server_metrics.csv.gz')

# Quick stats
print(df.info())
print(df.describe())

# Plot sample server CPU over time
server_sample = df[df['server_id'] == 'server_001']
server_sample.set_index('timestamp')['cpu_p95'].plot(figsize=(14,5), title='Server 001 CPU P95')
plt.show()

# Correlation heatmap
corr = df[['cpu_p95', 'memory_p95', 'disk_io_p95', 'network_p95']].corr()
sns.heatmap(corr, annot=True, cmap='coolwarm')
plt.title('Metric Correlations')
plt.show()
```

---

## Stage 3: Feature Engineering

### Features to Create

| Feature | Description | Config Reference |
|---------|-------------|------------------|
| `cpu_lag_1`, `cpu_lag_7`, `cpu_lag_30` | Lagged values | `feature_engineering.lags: [1, 7, 30]` |
| `cpu_rolling_mean_7`, `cpu_rolling_std_7` | Rolling statistics | `feature_engineering.rolling_windows: [7, 14, 30]` |
| `is_end_of_quarter` | Binary flag for Q-end proximity | `feature_engineering.external_regressors` |
| `is_month_end` | Binary flag | `feature_engineering.external_regressors` |
| `day_of_week` | 0-6 encoding | Time feature |
| `month` | 1-12 encoding | Time feature |
| `days_to_quarter_end` | Distance to next Q-end | Derived |

### Suggested Script: `src/feature_engineering.py`

---

## Stage 4: Model Training

### Models from Config

| Model | Status | Key Params |
|-------|--------|------------|
| **Prophet** | Enabled | `yearly_seasonality: true`, `weekly_seasonality: true` |
| **RandomForest** | Enabled | `n_estimators: 200`, `max_depth: 10` |
| **GradientBoosting** | Enabled | `n_estimators: 150`, `learning_rate: 0.05` |
| **AmazonForecast** | Disabled | Toggle on when ready |

### Training Strategy
1. Train per-server or per-archetype models
2. Use time-based train/test split (last 20% as holdout)
3. Cross-validate with 5 folds on training set

---

## Stage 5: Model Evaluation

### Metrics from Config
- **MAE** (Mean Absolute Error)
- **RMSE** (Root Mean Squared Error)
- **MAPE** (Mean Absolute Percentage Error)

### Focus Areas
- Overall accuracy across all servers
- **Peak period accuracy** (Q-end weeks) — critical for capacity planning
- Per-archetype performance (web vs database vs batch)

---

## Stage 6: Capacity Risk Flagging

### Thresholds from Config
```yaml
risk_analysis:
  high_risk_threshold: 80.0       # P95 % → flag as risk
  very_high_risk_threshold: 90.0
  underutilized_threshold: 40.0   # Candidate for consolidation
```

### Risk Categories
| Category | Condition | Action |
|----------|-----------|--------|
| **Very High Risk** | Predicted P95 > 90% within 90 days | Immediate upgrade |
| **High Risk** | Predicted P95 > 80% within 90 days | Plan upgrade |
| **Normal** | Predicted P95 < 80% | Monitor |
| **Underutilized** | Average < 40% | Consolidation candidate |

---

## Stage 7: Reporting / Dashboard

### Options
1. **Static Reports** — PDF/HTML via matplotlib + Jinja templates
2. **Streamlit Dashboard** — Interactive local app
3. **AWS QuickSight** — Native AWS BI tool, connects to Athena

### Key Visualizations
- Forecast vs actuals (with confidence intervals)
- Risk heatmap by server/archetype
- Quarterly trend analysis
- Underutilized server clustering (K-Means)

---

## Quick Reference: Key Commands

```bash
# Generate data (already done)
python src/data_generation.py

# Upload to S3 (Stage 1)
python scripts/push_to_s3.py

# Run feature engineering (Stage 3 - to be built)
python src/feature_engineering.py

# Train models (Stage 4 - to be built)
python src/train_models.py

# Generate risk report (Stage 6 - to be built)
python src/risk_analysis.py
```

---

## Files Reference

| File | Purpose |
|------|---------|
| `config/config.yaml` | Central configuration |
| `src/data_generation.py` | Generates synthetic data |
| `src/utils/server_archetypes.py` | Server behavior profiles |
| `scripts/push_to_s3.py` | Uploads data to S3 |
| `data/synthetic/server_metrics.csv.gz` | Generated dataset |

---

## Next Action

**Run Stage 1:**
```bash
python scripts/push_to_s3.py
```

Then proceed to Stage 2 (EDA notebook) to validate the data before feature engineering.
